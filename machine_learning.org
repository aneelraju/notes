* Coursera: Machine Learning - Andrew Ng, Stanford University
** Week1
*** Introduction
    + Machine Learning 
      - Is a science of getting computers to learn, without being
        explicitly programmed
      - examples: 
        - database mining : web clicks, medical record, biology, engineering
        - applications can't code by hand : autonomous helicopter, handwriting recognition, 
          natural language processing, computer vision
        - self-customizing programs : amazon, netflix
        - understanding human learning (brain, real AI)
    + supervised learning - "right answers" are given as data set
      + classification - discrete valued output (0,1,2...)
      + regression - continous value output
    + unsupervised learning - data doesn't have any label; find structure automatically
      + clustering - groups the data // news.google.com
      + eg:- organize computing clusters; social network analysis, market segmentation,
        astronomical data analysis
      + cocktail party problem algorithm // 2 microphone listens 2 speakers
        // one line algo:  [W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
        // svd - single value decomposition -> linear algebra routine built in ocative
        // single value decomposition
        + wiki link: https://en.wikipedia.org/wiki/Cocktail_party_effect
      + with unsupervised learning there is no feedback based on the prediction results   
    + octave - open source high level interpreted language
      + used for rapid prototyping and then migrate to c++ and java
      + https://www.gnu.org/software/octave/
    + prerequisite for this course
      + basic computer science principles and skills
      + basic probability theory
      + basic linear algebra
    + other matlab or octave are clojure, julia, python and R
    + sample problems or assignments link: http://cs229.stanford.edu/materials.html
*** Linear Regression with One Variable
    + model representation
      + supervised learning - given the "right answer" for each example in the data
      + regression problem - predict real-valued output
      + classfication problem - discrete valued output
      + training set -> learning algorithm -> h (hypothesis) (training)
        size of house -> h (hypothesis) -> estimated price (value out)
      + linear regression with one variable or univariate linear regression
        y = c + mx
      + when the target variable that we're trying to predict is continuouse we call
        the learning problem a regression problem
      + when y can take on only a small number of discrete values we call it a classification
        problem
    + cost function
      + 1/2m * sum of "m" training samples((h(x) - y)**2)
      + called as "cost function" or "squared error function" or "mean squared error"
      + most commonly used for regression problems
      + the mean is halved (1/2) as a convenience for the computation of the gradient
        descent, as the derivative term of the square function will cancel out the 1/2
        term

      
*** Linear Algebra Review
** Week2
*** Linear Regression with Multiple Variables
*** Octave/Matlab Tutorial
** Week3
*** Logistic Regression
*** Regularization
** Week4
*** Neural Networks:Representation
** Week5
*** Neural Networks:Learning
** Week6
*** Advice for Applying Machine Learning
*** Machine Learning System Design
** Week7
*** Support Vector Machines
** Week8
*** Unsupervised Learning
*** Dimensionality Reduction
** Week9
*** Anomaly Detection
*** Recommender Systems
** Week10
*** Large Scale Machine Learning
** Week11
*** Application Example: Photo OCR
** Additional


