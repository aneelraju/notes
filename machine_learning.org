* Coursera: Machine Learning - Andrew Ng, Stanford University
** Week1
*** Introduction
    + Machine Learning 
      - Is a science of getting computers to learn, without being
        explicitly programmed
      - examples: 
        - database mining : web clicks, medical record, biology, engineering
        - applications can't code by hand : autonomous helicopter, handwriting recognition, 
          natural language processing, computer vision
        - self-customizing programs : amazon, netflix
        - understanding human learning (brain, real AI)
    + supervised learning - "right answers" are given as data set
      + classification - discrete valued output (0,1,2...)
      + regression - continous value output
    + unsupervised learning - data doesn't have any label; find structure automatically
      + clustering - groups the data // news.google.com
      + eg:- organize computing clusters; social network analysis, market segmentation,
        astronomical data analysis
      + cocktail party problem algorithm // 2 microphone listens 2 speakers
        // one line algo:  [W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
        // svd - single value decomposition -> linear algebra routine built in ocative
        // single value decomposition
        + wiki link: https://en.wikipedia.org/wiki/Cocktail_party_effect
      + with unsupervised learning there is no feedback based on the prediction results   
    + octave - open source high level interpreted language
      + used for rapid prototyping and then migrate to c++ and java
      + https://www.gnu.org/software/octave/
    + prerequisite for this course
      + basic computer science principles and skills
      + basic probability theory
      + basic linear algebra
    + other matlab or octave are clojure, julia, python and R
    + sample problems or assignments link: http://cs229.stanford.edu/materials.html
*** Linear Regression with One Variable
    + model representation
      + supervised learning - given the "right answer" for each example in the data
      + regression problem - predict real-valued output
      + classfication problem - discrete valued output
      + training set -> learning algorithm -> h (hypothesis) (training)
        size of house -> h (hypothesis) -> estimated price (value out)
      + linear regression with one variable or univariate linear regression
        y = c + mx
      + when the target variable that we're trying to predict is continuouse we call
        the learning problem a regression problem
      + when y can take on only a small number of discrete values we call it a classification
        problem
    + cost function
      + 1/2m * sum of "m" training samples((h(x) - y)**2)
      + called as "cost function" or "squared error function" or "mean squared error"
      + most commonly used for regression problems
      + the mean is halved (1/2) as a convenience for the computation of the gradient
        descent, as the derivative term of the square function will cancel out the 1/2
        term
    + cost function - intuition II
      + contour plots or contour figures
        + is a graph that contains many contour lines. A contour line of a two variable
          function has a constant value at all points of the same line
    + gradient descent
      + to minimize cost function J(c, m)
      + start with some c0, m0 and keep change c0, m0 to reduce J(c0, m0) until we
        hopefully end up at a minimum
      + the way to find minimum is by taking the derivative of our cost function. the
        slope of the tangent is the derivative at that point and it will give us a
        direction to move towards. we make steps down the cost function in the direction
        with the steepest descent. the size of each step is determined by learning rate
      + if learning rate is too small, gradient descent can be slow; if learning rate is
        too large, gradient descent can overshoot the minimu. it may fail to converge
        or even diverge
      + as we approach a local minimum, gradient descent will automatically take smaller
        steps. so, no need to decrease learning rate over time
      + at local minimum, gradient descent will not change as slope is flat ('0')
      + if the slope of gradient descent is negative, the value of parameter (c, m) increases;
        if the slope of gradient descent is positive, the value of parameter (c, m) decreases
      + the cost function (convex quadratic function) always a bowl-shaped function, always converge to
        global minimum (no multiple local minimum)
      + "batch" gradient descent: each step of gradient descent uses all the training examples
*** Linear Algebra Review
    + matrix: rectangular array of numbers
    + dimension of matrix: number of rows x number of columns
    + M[i,j]: ith row, jth column
    + vector V: an nx1 matrix (only one column) of n dimensional
    + V[i] = ith element; 1-indexed vector start with index 1; 0-indexed vector start with index 0
    + Matrix addition
      + Mi + Mj = Mi11 + Mj11; Mi12 + Mj12; Mi21 + Mj21; Mi22 + Mj22; 
       you can add matrices of same dimensions only
    + scalar multiplication
      + 2 * M = 2*M11; 2*M12; 2*M21; 2*M22
    + Mi2x3 * Mj3x1 = Mi11*Mj11 + Mi12*Mj21 + Mi13*Mj31; Mi21*Mj11 + Mi22*Mj21 + Mi23*Mj31;
      + result V2x1 matrix
    + Mi2x3 * Mj3x2 = Mi11*Mj11 + Mi12*Mj21 + Mi13*Mj31; Mi21*Mj11 + Mi22*Mj21 + Mi23*Mj31;
                      Mi11*Mj12 + Mi12*Mj22 + Mi13*Mj32; Mi21*Mj12 + Mi22*Mj22 + Mi23*Mj32;
      + result M2x2 matrix
    + A * B != B * A (not commutative)
    + A*(B * C) == (A*B)*C (associative)
    + Identity matrix, I: diagonal is '1' rest elements are '0'
    + A*I = I*A = A
    + Matrix Inverse and Transpose
      + A*AInv = I
      + AInv calculated by octave
      + Matrix will all '0' doesnt have inverse
      + Matrix Transpose 
        + Let A be an mxn matrix and let B = A(T). Then B is an nxm matrix and Bij = Aji    
** Week2
*** Environment Setup instructions
    + matlab
      + https://www.mathworks.com/licensecenter/classroom/machine_learning_od/ // create account
      + download and install matlab
    + octave
      + https://support.apple.com/en-us/HT202491 // os gatekeeper
      + system perference -> security & privacy -> need to configure it to allow the Octave installer
      + http://sourceforge.net/projects/octave/files/Octave%20MacOSX%20Binary/2013-12-30%20binary%20installer%20of%20Octave%203.8.0%20for%20OSX%2010.9.1%20%28beta%29/GNU_Octave_3.8.0-6.dmg/download // download octave
      + follow installer's instructions
      + octave cli is needed to complete machine learning's programming assignments
        + octave includes experimental octave-gui but octave-cli is recommended as it's more stable
        + http://wiki.octave.org/Octave_for_MacOS_X#Package_Managers // install with brew
          + % brew tap homebrew/science
          + % brew update && brew upgrade
          + % brew install octave
            + % brew tap --repair // incase brew complains about not having a formula for ocatave
          + % brew update && brew upgrade // upgrades octave and its dependencies 
      + help
        + octave
          + % help // at octave-cli
          + link: http://www.gnu.org/software/octave/doc/interpreter/
        + matlab
          + % help <command> // at MATLAB command line
          + link: http://www.mathworks.com/help/matlab/
          + series of vidoes
            + introduction
              + http://youtu.be/rXwTiKGlilE // what is matlab
              + http://youtu.be/iYTzJXXI9vI // the matlab environment
              + http://youtu.be/jURDBsIPt5I // matlab variables
              + http://youtu.be/E7KllorEWkA // matlab as calculator
              + http://youtu.be/R-kBvJ3kVVk // math functions
            + vectors
              + http://youtu.be/2VNFqxmVqw8 // creating vectors via concatenation
              + http://youtu.be/GihLWwp8sBw // accessing elements of a vector
              + http://youtu.be/t9Kla_YFdfs // vector arithmetic
              + http://youtu.be/USehPX2iEa4 // vector transpose
              + http://youtu.be/L7cERR5J9XY // creating uniformly spaced vectors (the colon operator)
              + http://youtu.be/3QM3LRnb4Tw // creating uniformly spaced vectors (the LINSPACE func)
            + visualization
              + http://youtu.be/00k9A9W0cl8 // line plots
              + http://youtu.be/ab3XIDdloNI // annotating graphs
            + matrices and arrays
              + http://youtu.be/5tm6PKaJdI8 // creating matrices
              + http://youtu.be/DDnm7vek6KY // array creation functions
              + http://youtu.be/qqQnFp5aiuM // accessing elements of an array
              + http://youtu.be/SqvtT_VspKU // array size and lenght
              + http://youtu.be/TgopxS-_zl8 // concatenating arrays
              + http://youtu.be/-jgXqAYBhxI // matrix multiplication
            + programming
              + http://youtu.be/TZr6GyxnI_w // using MATLAB editor
              + http://youtu.be/5gVKJVVmbrM // logical operators
              + http://youtu.be/8wxh4LtT--g // conditional data selection
              + http://youtu.be/oaK2-ZT9dls // if else statements
              + http://youtu.be/1u3RahlWEZA // for loops
              + http://youtu.be/dofj51Ovdl4 // while loops
*** Multivariate Linear Regression
*** Computing Parameters Analytically
*** Submitting Programming Assignment
** Week3
*** Logistic Regression
*** Regularization
** Week4
*** Neural Networks:Representation
** Week5
*** Neural Networks:Learning
** Week6
*** Advice for Applying Machine Learning
*** Machine Learning System Design
** Week7
*** Support Vector Machines
** Week8
*** Unsupervised Learning
*** Dimensionality Reduction
** Week9
*** Anomaly Detection
*** Recommender Systems
** Week10
*** Large Scale Machine Learning
** Week11
*** Application Example: Photo OCR
** Additional


