* containers notes
** docker
*** udemy - docker mastery: the complete toolset from a docker captain - bret fisher
**** course introduction and docker setup
***** getting course resources
      + resources
        + https://github.com/bretfisher/udemy-docker-mastery - course repository
***** installing docker: the fast way
      + installation
        + ubuntu:xenial64
 	  + install docker-ce
  	    + % sudo apt-get remove docker docker-engine docker.io; # remove old versions
            + % sudo apt-get update; # update the apt package index
	    + % sudo apt-get install apt-transport-https ca-certificates curl software-properties-common; # install packages to allow apt to use repository over HTTPS
	    + % curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -; # add docker's official GPG key
	    + % sudo apt-key fingerprint 0EBFCD88; # verify key with the fingerprint
	    + % sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"; # set up stable repository
	    + % sudo apt-get update; # update the apt package index
	    + % sudo apt-get install docker-ce; # install latest version of docker ce
	    + % sudo docker run hello-world
	    + to avoid typing sudo (not recommended)
	      + % sudo usermod -aG docker <user>; # add <user> to docker group
	      + relogin
          + install docker-compose
	    + % sudo curl -L https://github.com/docker/compose/releases/download/1.17.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose; # download and install docker compose
	    + % sudo chmod +x /usr/local/bin/docker-compose
	    + % docker-compose --version
	    + % sudo curl -L https://raw.githubusercontent.com/docker/compose/1.17.0/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose; # Place following script in /etc/bash_completion.d/
	    + re-login
          + install docker-machine
	    + % curl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` > /tmp/docker-machine ; # downlaod docker machine and extract to your PATH
            + % sudo chmod +x /tmp/docker-machine
            + % sudo cp /tmp/docker-machine /usr/local/bin/docker-machine
	    + % docker-machine version; # check docker machine
	    + % scripts=( docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash ); for i in "${scripts[@]}"; do sudo wget https://raw.githubusercontent.com/docker/machine/v0.13.0/contrib/completion/bash/${i} -P /etc/bash_completion.d; done
	    + re-login
	  + because docker-compose and docker-machine are installed manually, make sure you check for latest version frequently
      + run more nodes: % docker-machine create --driver
        + virtualbox, aws etc. https://docs.docker.com/machine/drivers/
      + resources
        + https://labs.play-with-docker.com - run one or more docker instances inside your browser
        + https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/ - ubuntu docker installation
        + https://get.docker.com; # install docker via running script
	  + % curl -fsSL get.docker.com -o get-docker.sh
	  + % sh get-docker.sh
        + https://docs.docker.com/machine/install-machine/ - docker machine install (docker docs)
        + https://docs.docker.com/compose/install/ - docker compose install (docker docs)
        + www.bretfisher.com/shell - customize shell      
***** docker for windows 10 pro/ent: setup and tips
      + resources
        + https://store.docker.com/editions/community/docker-ce-desktop-windows - download docker ce for windows
        + https://desktop.github.com - download github desktop
        + https://code.visualstudio.com - download visual studio code
        + https://docs.docker.com/docker-for-windows/#explore-the-application-and-run-examples - setup tab completion for powershell
        + http://cmder.net - download cmder
        + https://docs.docker.com/docker-for-windows/faqs/ - docker for windows FAQ
***** docker for mac setup and tips
      + resources
        + https://store.docker.com/editions/community/docker-ce-desktop-mac  - docker ce for mac
        + https://www.iterm2.com - iterm2
        + https://docs.docker.com/docker-for-mac/#installing-bash-completion - installing bash completion on mac
        + https://brew.sh - installing homebrew (the brew cli)
***** docker for linux setup and tips
      + resources
        + https://docs.docker.com/compose/install/ - install docker compose 
        + https://docs.docker.com/machine/install-machine/ - install docker machine
**** creating and using containers like a boss
***** check our docker install and config
      + % sudo docker version; # gives version of client and server
      + % sudo docker info; # gives config and setup
      + % sudo docker; # gives list of commands and management commands
      + new syntax "sudo docker <management-commands> <sub-commands>"
***** starting a Nginx web server
      + hub.docker.com - default image "registry"
      + % sudo docker container run --publish 80:80 nginx; # run nginx container
	+ go to "localhost" in browser
	+ "--publish" opened port 80 on the host IP and routes that traffic to the container IP, port 80
	+ % sudo docker container run --publish 80:80 --detach nginx; # to run in background
      + % sudo docker container ls; # lists running containers
      + % sudo docker container stop <container_id>; # stop container
      + % sudo docker container ls -a; # 
      + % sudo docker container run --publish 80:80 --detach --name webhost nginx; # assing container name
      + % sudo docker container logs webhost; # shows logs
      + % sudo docker container top webhost; # lists process running inside container
      + % sudo docker container --help; # list of container commands
      + % sudo docker container rm <container id1> <container id2>; # removes stopped containers
      + % sudo docker container rm -f <container id1> <container id2>; # removes running and stopped containers
***** debrief: what happens when we run a container
      + % sudo docker container run --publish 8080:80 --name webhost -d nginx:1.11 nginx -T; # run nginx version 1.11 and command "nginx -T"
***** container vs vm: it's just a process
      + % sudo docker run --name mongo -d mongo; # run mongodb
      + % sudo docker ps; # lists running containers
      + % ps aux; # lists all running hosts process
      + % ps aux | grep mongo; # filter running mongo process
      + % sudo docker start mongo; # start mongo container
      + resources
        + https://www.youtube.com/watch?v=sK5i-N34im8&feature=youtu.be&list=PLBmVKD7o3L8v7Kl_XXh3KaJl9Qw2lyuFl - docker internals
***** assignment answers: manage multiple containers
      + docs.docker.com and --help are your friend
      + % sudo docker container run -d -p 3306:3306 --name db -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql; # run mysql db, --e is to pass environment variables
      + % sudo docker container logs db; # check for "GENERATED ROOT PASSWORD" from mysql logs
      + % sudo docker container run -d --name webserver -p 8080:80 httpd; # run apache webserver
      + % sudo docker ps; # checker item PORTS for each container
      + % sudo docker container run -d --name proxy -p 80:80 nginx; # run nginx to act as proxy
      + % sudo docker container ls; # same as "sudo docker ps"
      + % curl localhost; # should give nginx
      + % curl localhost:8080; # should give apache
      + % sudo docker container stop <proxy cont id> <webserver cont id> <db cont id>; # you can check tab completion after "sudo docker container stop <tab>"
      + % sudo docker ps -a
      + % sudo docker container rm <proxy cont id> <webserver cont id> <db cont id>
      + % sudo docker ps -a; # everything is cleaned up
      + % sudo docker image ls; # images are still present
***** what's going on in containers: CLI process monitoring
      + % sudo docker container run -d --name nginx nginex; # run nginx
      + % sudo docker run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql; # run mysql
      + % sudo docker container ls
      + % sudo docker container top mysql; # process list in one container
      + % sudo docker container top nginx; # process list in one container
      + % sudo docker container inspect mysql; # details of one container config; shows metadata about the container (startup config, volumes, networking etc)
      + % sudo docker container stats --help; # displays help
      + % sudo docker container stats; # show live performance data for all containers
***** getting a shell inside containers: no need for ssh
      + % sudo docker container run -it --name proxy nginx bash; # start new container interactively, no ssh needed; '-t'='--tty' -> simulates a real terminal like ssh; '-i'='--interactive' -> keep session open to receive terminal input; bash shell if run with -it, it will give you a terminal inside the running container
        + we can do additional things from shell like installing additional packages and etc; '% exit' to exit from shell and container stops
      + % sudo docker container run -it --name ubuntu ubuntu; # full distribution of linux
        + c% apt-get update; # update packages inside ubuntu container
        + c% apt-get install -y curl; # install curl
        + c% curl www.google.com; # running curl inside container
        + c% exit; exit and stops container
      + % sudo docker start -ai ubuntu; # start ubuntu and curl is installed; new run container will not have curl installed; '-a'='--attach', 'i'='--interactive'
      + % sudo docker start --help; #  start command help
      + % sudo docker exec --help; #  exec command help
      + % sudo docker exec -it mysql bash; # run additional command in existing container; bash shell on running container mysql; 
        c% ps aux; # process list inside container
        c% exit; # exit container
      + 'alpine' is another distribution of linux which is a small security focused distribution
      + sudo docker pull alpine; # pull alpine image, has it's own package manager
      + sudo docker container run -it alpine bash; # throws error as bash is not installed
      + sudo docker container run -it alpine sh; # but has shell; package manager is 'apk' which can be used to install bash
      + resources
        + https://www.digitalocean.com/community/tutorials/package-management-basics-apt-yum-dnf-pkg - package management basics:apt,yum,dnf,pkg
***** docker networks: concepts for private and public comms in containers
      + docker networks defaults
        + each container connected to a private virtual network 'bridge'
        + each virtual network routes through NAT firewall on host IP; NAT firewall is docker daemon configuring host IP on its default i/f so that your containers can access internet 
        + all containers on a virtual network can talk to each other without -p
        + best practice is to create a new virtual network for each app
          + network "my_web_app" for mysql and php/apache containers
          + network "my_api" for mongo and nodejs containers
        + defaults work well in many cases, but easy to swap out parts to customize it
        + make new virtual networks and attach containers to more than one virtual network
        + skip virtual networks and use host IP (--net=host)
        + use different docker network drivers to gain new abilities
      + 'docker container run -p' - exposes ports on your machine
      + % sudo docker container run -p 80:80 --name webhost -d nginx; # -p (--publish) publishing port in HOST:CONTAINER format
      + % sudo docker container port webhost; # displays port
      + % sudo docker container inspect --format '{{ .NetworkSettings.IPAddresss }}' webhost; # --format - a common option for formatting the output of commands using "Go templates"; display container ip address which is different that host IP
      + % ifconfig en0; # mac ip address
      + resources
        + https://docs.docker.com/engine/admin/formatting/ - docker's --format for filtering cli output
***** docker networks: cli management of virtual networks
      + a recent june 2017 change, removes ping in nginx; so replace "nginx" to "nginx:latest" which still has ping
      + running docker inside docker (mac)
        + % docker run -v /var/run/docker.sock:/var/run/docker.sock -it ubuntu bash; # run ubuntu container sharing that sock
        + c% apt-get update && apt-get install curl
        + c% curl -sSL https://get.docker.com/ | sh
        + c% docker version
        + c% docker container run hello-world
        + c% exit
      + % sudo docker network ls; # lists all created networks: bridge, host, null; '--network bridge' is default docker virtual network which is NAT'ed behind the HOST IP; '--network host' is gains performance by skipping virtual networks but sacrifices security of container model; '--network none' removes eth0 and only leaves you with localhost interface in container
      + % sudo docker network inspect bridge; # can see list of containers attached to it; IPAM shows default IP's assigned
      + % sudo docker network create my_app_net; # spawns a new virtual network for you to attach containers to
      + % sudo docker network ls; # shows list of networks
      + % sudo docker network --help; # network command help
      + % sudo docker container run -d --name new_nginx --network my_app_net nginx
      + % sudo docker network inspect my_app_net; # new_nginx is on that network
      + % sudo docker network connect <network id1> <network id2>; # connect dynamically creates a NIC in a container on an existing virtual network
      + % sudo docker network inspect <network id1>; # you can see both networks
      + % sudo docker network disconnect <network id1> <network id2>; # disconnect dynamically rmeoves a NIC from a container on a specific virtual network
      + create your apps so frontend/backend sit on same docker network and their inter-communication never leaves host. all externally exposed ports closed by default and must manually expose via -p which is better default security
***** docker networks: dns and how containers find each other
      + static IP's and using IP's for talking to containers is an anti-patern. do your best to avoid it
      + docker daemon has a built in DNS server that containers use by default
      + % sudo docker container ls
      + % sudo docker network inspect <my_app_net network id>; # not default network brigde
      + dns default names, docker defaults the hostname to the container's name, but you can also set aliases
      + % sudo docker container run -d --name my_nginx --network my_app_net; # my_app_net contains two containers
      + % sudo docker container exec -it my_nginx ping new_nginx; # ping new_nginx from my_nginx
      + % sudo docker container exec -it new_nginx ping my_nginx; # ping my_nginx from new_nginx
      + % sudo docker network ls; # default bridge doesn't have default dns server, need to use --link, it's easier to create new network than bridge -l; docker compose makes it easier for dns
***** assignment answers: using containers for cli testing
      + % sudo docker container run --rm -it centos:7 bash
        + c% yum update curl
        + c% curl --version
        + c% exit
      + % sudo docker container run --rm -it ubuntu:14.04 bash
        + c% apt-get update && apt-get install -y curl
        + c% curl --version
        + c% exit
      + % sudo docker ps -a; # both centos and ubuntu disappears
***** assignment answers: dns round robin test
      + From docker engine 1.11, we can have multiple containers on a created network respond to the same DNS address
      + 'elasticsearch' is becoming popular, is RESTful search and analytics engine
      + % sudo docker network create dude
      + % sudo docker container run -d --net dude --net-alias search elasticsearch:2
      + % sudo docker container run -d --net dude --net-alias search elasticsearch:2
      + % sudo docker container ls
      + % sudo docker container run --rm --net dude alpine nslookup search; # shows two dns entries
      + % sudo docker container run --rm --net dude centos curl -s search:9200; # gives first container
      + % sudo docker container run --rm --net dude centos curl -s search:9200; # gives second container in random fashion
      + % sudo docker container ls
      + % sudo docker container rm <container 1> <container 2>
**** container images, where to find them and how to build them
***** what's an image (and what isn't)
      + resources
        + https://github.com/moby/moby/blob/master/image/spec/v1.md - official docker image specification
***** the mighty hub: using docker hub registry images
      + 'https://hub.docker.com' # docker hub
        + official images are great way to start
      + % sudo docker pull nginx; # pulls nginx latest version
      + % sudo docker pull nginx:1.11.9; # pulls nginx 1.11.9 version, especially needed for production development
      + % sudo docker pull nginx:1.11.9-alpine; # 'alpine' is linux distribution which is very small
      + resources
        + https://github.com/docker-library/official-images/tree/master/library - list of official docker images
***** images and their layers: discover the image cache
      + % sudo docker image history nginx:latest; # history show layers of changes made in image
      + % sudo docker image inspect nginx; # inspect returns JSON metadata about the image; gives details of image
      + images are made up of file system changes and metadata; a container is just a single read/write layer on top of image
      + resources
        + https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/ - images and containers from docker docs
***** image tagging and pushing to docker hub
      + % sudo docker image tag --help; # tag assign one or more tags to an image; images are referred as <user>/<repo>:<tag>
      + office repositories live at the "root namespace" of the registry, so they don't need account name in front of repo name
      + % sudo docker pull mysql/mysql-server; # pulls mysql/mysql-server image
      + % sudo docker image tag nginx aneelraju/nginx; # rename image 'nginx' to 'aneelraju/nginx; syntax 'sudo docker image tag 'SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]''
      + % sudo docker login; # 'sudo docker login <server>' defaults to logging in hub, but you can override by adding server url
        + type username and password; # wrote to file ~/.docker/config.json
      + % sudo docker image push aneelraju/nginx; # push uploads changed layers to a image registry (default is hub)
      + % sudo docker image tag aneelraju/nginx aneelraju/nginx:testing
      + % sudo docker image push aneelraju/nginx:testing
      + % sudo docker logout; # always logout from shared machines or servers when done, to protect your account
***** building images: the dockerfile basics
      + 'sudo docker image build -f some-dockerfile'; # builds docker image
      + package manager like apt and yum are one of the reasons to build containers FROM Debian, Ubuntu, Fedora and CentOS
      + environment variables are one reason they were chosen as preferred way to inject key/value is they work everywhere
      + % cp material/udemy-docker-mastery/dockerfile-sample-1 work/. && cd work/dockerfile-sample-1
      + % sudo docker image build -t customnginx .; # builds docker image in current directory
***** building images: extending official images
      + % sudo docker container run -p 80:80 --rm nginx; # default behavior
      + % cp material/udemy-docker-mastery/dockerfile-sample-2 work/. && cd work/dockerfile-sample-2
      + % sudo docker image build -t nginx-with-html .; # as much as possible use popular docker images from docker hub
***** assignment answers: build your own image
      + Dockerfiles are part process workflow and part art
      + % cp material/udemy-docker-master/dockerfile-assignment-1 work/. && cd work/dockerfile-assignment-1
      + prepare dockerfile
      + 'CMD ["executable", "param1", "param2"]'
      + % sudo docker image build -t testnode .
      + % sudo docker container run --rm -p 80:3000 testnode
      + % sudo docker image tag testnode aneelraju/testing-node
      + % sudo docker push aneelraju/testing-node
      + % sudo docker image ls
      + % sudo docker image rm aneelraju/testing-node
      + % sudo docker container run --rm -p 80:3000 aneelraju/testing-node; # downloads from docker hub
**** container lifetime & persistent data: volumes, volumes, volumes
***** container lifetime & persistent data
      + container are usually immutable and ephemeral; "immutable infrastructure": only re-deploy containers, never change
      + container should not mix data with binaries; docker gives us features to ensure these "separation of concerns"
      + for "persistent data" two solutions are available: volumes and bind mounts
      + volumes: make special location outside of container UFS (union file system); container see as a local file path
      + bind mounts: link container path to host path
      + resources
        + https://oreilly.janrainsso.com/static/server.html?origin=https%3A%2F%2Fwww.oreilly.com%2Fideas%2Fan-introduction-to-immutable-infrastructure - intro to immutable infrastructure concepts
        + https://12factor.net - the 12-factor app (everyone should read: key to cloud native app design, deployment and operation)
        + https://medium.com/@kelseyhightower/12-fractured-apps-1080c73d481c - 12 fractured apps (a follow-up to 12-factor, a greate article on how to do 12F correctly in containers)
***** persistent data: data volumes
      + VOLUME command in Dockerfile; # search Dockefile for official mysql in docker file (best way to learn dockerfile best practices); creates and assign volumes; volumes needs manual deletion
      + % sudo docker pull mysql
      + % sudo docker image inspect mysql; # under Config check for "Volumes"
      + % sudo docker container run -d --name mysql -e MYSQL_ALLOW_EMTPY_PASSWORD=True mysql
      + % sudo docker container inspect mysql; # inspect for Mounts and Volumes
      + % sudo docker volume ls; # shows one volume
      + % sudo docker volume inspect <volume name>
      + % sudo docker container run -d --name mysql2 -e MYSQL_ALLOW_EMTPY_PASSWORD=True mysql
      + % sudo docker volume ls; # shows two volumes
      + % sudo docker container stop mysql
      + % sudo docker container stop mysql2
      + % sudo docker container ls
      + % sudo docker container ls -a
      + % sudo docker volume ls; # volumes still present
      + % sudo docker container rm mysql mysql2
      + % sudo docker volume ls; # volumes still present; data still safe
      + 'named volumes' are friendly way to assing vols to containers
      + % sudo docker container run -d --name mysql -e MYSQL_ALLOW_EMTPY_PASSWORD=True -v mysql-db:/var/lib/mysql mysql; # mysql-db is named volume
      + % sudo docker volume ls
      + % sudo docker volume inspect mysql-db
      + % sudo docker container rm -f mysql; # remove container
      + % sudo docker container run -d --name mysql3 -e MYSQL_ALLOW_EMTPY_PASSWORD=True -v mysql-db:/var/lib/mysql mysql
      + % sudo docker volume ls; # doesn't create new volume
      + % sudo docker container inspect mysql3; # name friendly
      + % sudo docker volume create --help; # 'docker volume create' required to do this before "docker run" to use custom drivers and labels
      + sometime you need to create volumes ahead of run, usually for local development specifying in Dockerfile or in run command is fine
***** persistent data: bind mounting
      + mapping a host file or directory to a container file or directory; basically just two locations pointing to the same files
      + can't use in Dockerfile, must be at container run as bind mount are host specific
      + bind mounts starts with '/' and full path
      + % cd work/dockerfile-sample-2
      + % sudo docker container run -d --name nginx -p 80:80 -v $(pwd):/usr/share/nginx/html nginx; # bind mounts current directory
      + % sudo docker container exec -it nginx bash; # get shell into container
      + c% cd /usr/share/nginx/html; # we can host files 
***** assignment answers: database upgrades with named volumes
      + get volume path from docker hub postgres:9.6.1
      + % sudo docker container run -d --name psql -v psql:/var/lib/postgresql/data postgres:9.6.1
      + % sudo docker container logs -f psql; # -f keeps watching as it runs
      + ctrl-c
      + % sudo docker container stop <container id>
      + % sudo docker container run -d --name psql2 -v psql:/var/lib/postgresql/data postgres:9.6.2
      + % sudo docker container ps -a
      + % sudo docker volume ls; # one volume psql
      + % sudo docker container logs <container id>; # postgress is successfully upgraded
***** assignment answers: edit code running in containers with bind mounts
      + good for local development and local testing (similar to vagrant /vagrant folder); allows you to edit files on host machine
      + "Jekyll" is static site generator; # for a simple web site
      + % cp material/udemy-docker-mastery/bindmount-sample-1 work/. && cd work/bindmount-sample-1
      + % sudo docker run -p 80:4000 -v $(pwd):/site bretfisher/jekyll-serve
      + resources
        + https://jekyllrb.com - jekyll, a static site generator
        + https://hub.docker.com/r/bretfisher/jekyll-serve/ - info about how to use jekyll in a docker container for easy SSG development
**** making it easier with docker compose: the multi-container tools
***** docker compose and the docker-compose.yml file
      + docker componse is a cli tool and configuration file; it is comprised of 2 separate but related things
        + 1. YAML formatted file that describes our solution options for: containers, networks and volumes
        + 2. A cli tool docker-compose used for local dev/test automation with those YAML files
      + % sudo docker-compose --help; # docker compose help
      + % cp material/udemy-docker-mastery/compose-sample-1 work/. && cd work/compose-sample-1
      + go through template.yml, docker-compose.yml, compose-2.yml, compose-3.yml
      + resources
        + http://www.yaml.org/start.html - the yaml format: sample generic yaml file
        + http://www.yaml.org/refcard.html - the yaml format: quick reference
        + https://docs.docker.com/compose/compose-file - docker compose file
        + https://docs.docker.com/compose/compose-file/compose-versioning/ - compose file version differences (docker docs)
        + https://github.com/docker/compose/releases - docker compose release downloads (good for linux users that need to download manually)
        + https://wordpress.com - website using wordpress
        + https://ghost.org - open source professional publishing platform
***** trying out basic compose commands
      + docker-compose cli is not a production-grade tool but idea for local development and test
      + 'docker-compose up' - setup volumes/networks and start all containers
      + 'docker-compose down' - stop all containers and remove cont/vol/net
      + if your projects had a Dockerfile and docker-compose.yml then "new developer onboarding" would be
        + 'git clone github.com/some/software'
        + 'docker-compose up'
      + % cp material/udemy-docker-mastery/compose-sample-2 work/. && cd work/compose-sample-2
      + % sudo docker-compose up; # docker-compose is not installed by default on linux with docker
        + % ctrl-c
      + % sudo docker-compose -d; # to run it in the background
      + % sudo docker-compose logs; # to see logs
      + % sudo docker-compose --help; # docker-compose help
      + % sudo docker-compose ps; # list of running containers
      + % sudo docker-compose top; # streaming running process
      + % sudo docker-compose down; # stop and clean-up running processes
      + resources
        + https://github.com/docker/compose/releases - docker-compose download for linux via github, win/mac already have it
***** assignment answers: build a compose file for a multi-container service
      + 'drupal' is content management system website
      + % cp material/udemy-docker-mastery/compose-assignment-2 work/. && cd work/compose-assignment-2
      + % sudo docker pull drupal
      + % sudo docker image inspect drupal
      + check for exposed ports
      + compose docker-compose.yml file
      + % sudo docker-compose up
      + % sudo docker-compose down -v; # remove volumes as well
      + resources
        + https://www.drupal.org; # opensource content management framework
***** adding image building to compose files
      + % cp material/udemy-docker-mastery/compose-sample-3 work/. && cd work/compose-sample-3
      + % sudo docker-compose up
      + % sudo docker-compose down -rmi local; # remove volumes and images as well
      + resources
        + https://docs.docker.com/compose/compose-file/#build - (docker docs) compose file build options
***** assignment answers: compose for run-time image building and multi-container dev
      + % cp work/compose-assignment-2 work/compose-assignment-3 && cd work/compose-assignment-3
      + edit docker-compose.yml and Dockerfile according to README.md
      + % docker-compose up -d
      + configure drupal and install bootstrap theme
      + % docker-compose down
      + % docker-compose up; # website starts directly without installation process
**** docker services and the power of swarm: built-in orchestration
***** swarm mode: built-in orchestration
      + swarm mode is a clustering solution built inside docker
      + swarm commands (docker swarm/node/service/stack/secret) aren't enabled by default
      + docker service (replaces docker run command on swarm)
        + manager node:
          + RAFT
            + API - Accepts command from client and creates service object
            + Orchestrator - Reconcillation loop for service objects and creates tasks
            + Allocator - Allocates IP addresses to tasks
            + Scheduler - Assigns nodes to tasks
            + Dispatcher - Checks in on workers
        + Worker Node
          + Worker - Connects to dispatcher to check on assigned tasks
          + Executor - Executes the tasks assigned to worker node
      + resources
        + https://www.youtube.com/watch?v=dooPhkXT9yI - docker 1.12 swarm mode deep dive part 1: topology (YouTube)
        + https://www.youtube.com/watch?v=dooPhkXT9yI - docker 1.12 swarm mode deep dive part 2: topology (YouTube)
        + https://speakerdeck.com/aluzzardi/heart-of-the-swarmkit-topology-management - Heart of the SwarmKit: Topology Management (slides)
        + https://www.youtube.com/watch?v=EmePhjGnCXY - Heart of the SwarmKit: Store, Topology & Object Model (YouTube)
        + http://thesecretlivesofdata.com/raft/ - Raft Consensus Visualization (Our Swarm DB and how it stays in sync across nodes)
***** create your first service and scale it locally
      + % sudo docker info; # Displays systemwide info; check for Swarm: inactive
      + % sudo docker swarm init; # create a single node cluster on localhost and make current node as manager
        + lots of PKI and security automation
          + root signing certificate created for our swarm
          + certificate is issued for first manager node
          + join tokens are created
        + raft database created to store root CA, configs and secrets
          + encrypted by default on disk (1.13+)
          + no need for another key/value system to hold orchestration/secrets
          + replicates logs amongst managers via mutual TLS in "control plane"
          + raft ensures consistency esp for cloud deployment
      + % sudo docker node ls; # lists nodes
      + % sudo docker node --help; # docker node help
      + % sudo docker swarm --help; # docker swarm help
      + % sudo docker service --help; # docker service help; # 'docker run' is designed for single cluster and 'docker service' is for multi-node clusters
      + % sudo docker service create alpine ping 8.8.8.8; # gives service id not container id
      + % sudo docker service ls; # lists services
      + % sudo docker service ps <servicename>
      + % sudo docker service update <serviceid> --replicas 3; # scaling up services
      + % sudo docker service ls
      + % sudo docker service ps; # 3 services are run
      + % sudo docker update --help; # docker update help; update configuration of one or more containers
      + % sudo docker service update --help; # update a service without taking service down
      + % sudo docker container rm -f <containerid>; # container is removed but it is removed from behind, service will launch new service
      + % sudo docker service rm <servicename>; # removes containers through service
      + % sudo docker service ls; # service is removed
      + % sudo docker container ls; # containers are removed
      + resources
        + https://docs.docker.com/engine/swarm/services/ - deploy services to a swarm (docker docs)
***** creating a 3-node swarm cluster
      + 'play-with-docker.com'; # only needs a browser, but resets after 4 hours
        + quickly create nodes (3 nodes) and each nodes can talk to each other
        + useful if have slower machine and try quickly online
        + runs docker inside docker and has latest tools installed
      + docker-machine + virtualbox; # free and runs locally, but requires a machine with 8GB memory
        + % sudo docker-machine --help; # docker-machine help
        + % sudo docker-machine create node1; # create node through virtual busy box (light weight linux machine)
        + % sudo docker-machine ssh node1; # ssh to node1
          + or % sudo docker-machine env node1; # displays env
          + % eval $(docker-machine env node1); # copy on command prompt
          + docker terminal commands on node1
      + roll you own
        + docker-machine can provision machines for amazon, azure, do, google etc
        + install docker anywhere with get.docker.com; # preferred method as docker-machine may not be used in production settings
      + digital ocean + docker install
        + cheapest and easiest to start digital service
        + most like a production setup, but costs $5-10/node/month
        + create 3 droplets (node1,node2,node3) on digital ocean with 1vCPU + 1GB standard machine ($10/node/month)
        + % ssh root@<node1ip>
        + node1% curl -sSL https://get.docker.com/ | sh; # install docker
        + % ssh root@<node2ip>
        + node2% curl -sSL https://get.docker.com/ | sh; # install docker
        + % ssh root@<node3ip>
        + node3% curl -sSL https://get.docker.com/ | sh; # install docker
        + node1% sudo docker swarm init --advertise-addr <node1 ip>; # docker swarm init
        + node2% sudo docker swarm join --token SWMTKN-1-2th56plepacrp8d8giiyqrtv86m803ia7kiecrzog9xhd9z26h-1mhl4b3x04l2vutj87olww5cn <node1ip>:2377
        + node3% sudo docker swarm join --token SWMTKN-1-2th56plepacrp8d8giiyqrtv86m803ia7kiecrzog9xhd9z26h-1mhl4b3x04l2vutj87olww5cn <node1ip>:2377
        + node1% sudo docker node ls; # lists 3 nodes where node1 is manager and node 2 and node3 are workers
        + node2% sudo docker node ls; # errors out; node2 and node3 are workers so can't run swarm commands
        + node1% sudo docker node update --role manager node2; # upgrading node2 to manager
        + node2% sudo docker node ls
        + node1% sudo docker swarm join-token manager; # get manager token
        + node3% docker swarm join --token SWMTKN-1-2th56plepacrp8d8giiyqrtv86m803ia7kiecrzog9xhd9z26h-e9fhwpe6yhxb8boa2tix8nv6l <node1ip>:2377; # adding node3 directly as manager
        + node1% sudo docker node ls
        + node1% sudo docker service create --replicas 3 alpine ping 8.8.8.8
        + node1% sudo docker service ls; # list of services
        + node1% sudo docker node ps; # list container runs on node1
        + node1% sudo docker node ps node2; # list container runs on node2
        + node1% sudo docker service ps <service name>; # list of container runs, this is due to routing mesh
        + node1% sudo docker service inspect drupal; # list only node1 ip overlay
        + node1% sudo docker node ls
        + node1% sudo docker node demote node3
        + node3% sudo docker swarm leave
        + node3% sudo docker node rm node3
        + node1% sudo docker node ls
        + node1% sudo docker node demote node2
        + node2% sudo docker swarm leave
        + node2% sudo docker node rm node2
        + node1% sudo docker swarm leave --force; # use --force only on last manager node to leave
      + resources
        + https://www.digitalocean.com/?refcode=b813dfcad8d4&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=CopyPaste - digital ocean coupon for $10
        + https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets - create and upload a SSH key to digital ocean
        + https://www.bretfisher.com/docker-swarm-firewall-ports/ - docker swarm firewall ports
        + https://www.digitalocean.com/community/tutorials/how-to-configure-custom-connection-options-for-your-ssh-client - configure SSH for saving options for specific connections
***** scaling out with overlay networking
      + '--driver overlay' - creates swarm wide bridge network
      + only for container-to-container traffic inside a single swarm
      + optional IPSec (AES) encryption on network creation
      + each service can be connected to multiple networks (e.g. front-end, back-end)
      + node1% sudo docker network create --driver overlay mydrupal
      + node1% sudo docker network ls
      + node1% sudo docker service create --name psql --network mydrupal -e POSTGRES_PASSWORD=mypasswd postgres; # service can be launched on either of node1, node2 or node3
      + node1% sudo docker service ls
      + node1% sudo docker service ps psql; # psql running on node2
      + node2% sudo docker container logs <container name>
      + node1% sudo docker service create --name drupal --network mydrupal -p 80:80 drupal
      + node1% sudo docker service ls
      + node1% sudo docker service ps drupal; # drupal running on node1
      + node1% watch sudo docker service ls; # linux command watch, to watch services
      + in browser https://<node1ip>; # drupal install; overlay acts like every thing on same sub net
      + all 3 (node1ip, node2ip and node3ip) ip's display drupal website
***** scaling out with routing mesh
      + routing mesh
        + routes ingress (incoming) packets for a service to proper task
        + spans all nodes in swarm
        + uses IPVS from linux kernel
        + load balances swarm services across thier tasks
        + two way this works:
          + container-to-container in a overlay network (uses VIP)
          + external traffic incoming to published ports (all nodes listen)
        + this is stateless load balancing
        + this LB is at OSI layer 3 (TCP), not Layer 4 (DNS)
        + both limitation can be overcome with
          + Nginx or HAProxy LB proxy
          + Docker EE, which comes with built-in L4 web proxy
      + node1% sudo docker service create --name search --replicas 3 -p 9200:9200 elasticsearch:2
      + node1% sudo docker service ps search; # list search ps
      + node1% curl localhost:9200; # elasticsearch basic info
      + node1% curl localhost:9200; # now on different node showing load balancing
      + resources
        + https://docs.docker.com/engine/swarm/ingress/ - use swarm mode routing mesh (docker docs)
***** assignment answers: create a multi-service multi-node web app
      + docker's distributed voting app
      + % cp material/udemy-docker-mastery/swarm-app-1 work/. && cd work/swarm-app-1
      + 1 volume, 2 networks and 5 services needed
      + create the commands needed, spin up services and test app
      + everything is using docker hub images, so no data needed on swarm; don't build on production swarms/cloud as they eat resources)
      + like many computer things, this is 1/2 art and 1/2 science
      + node1% sudo docker service ls; # no services
      + node1% sudo node ls; # node1, node2 and node3 available as managers
      + edit README.md and following instructions
      + node1% sudo docker network create -d overlay backend; # create backend network
      + node1% sudo docker network create -d overlay frontend; # create frontend network
      + node1% sudo docker service create --name vote -p 80:80 --network frontend --replicas 2 dockersamples/examplevotingapp_vote:before
      + node1% sudo docker service create --name redis --network frontend --replicas 1 redis:3.2
      + node1% sudo docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker
      + node1% sudo docker service create --name db --network backend --mount type=volume,source=db-data,target=/var/lib/postgresql/data postgres:9.4
        + '-v' is not support in docker service, it's replaced with '--mount'
      + node1% sudo docker service create --name result --network backend -p 5001:80 dockersamples/examplevotingapp_result:before
      + node1% sudo docker service ls; # all five services are running
      + node1% sudo docker service ps db; # runs on node3
      + node1% sudo docker service ps redis; # runs on node3
      + node1% sudo docker service ps result; # runs on node2
      + node1% sudo docker service ps worker; # runs on node2 and node1
      + node1% sudo docker service logs worker; # runs on node1; worker fails until db is up but service is re-started
      + node1% sudo docker container logs <workercontainername>
      + In browser <node1ip> to vote and <node2ip>:5001 to check result
***** swarm stacks and production grade compose
      + stacks: production grade compose
        + stacks accpt compose files as their declarative definition for services, networks and volumes
        + 'docker stack deploy' rather then docker service create
        + stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name
        + new 'deploy:' key in compose file. can't do build: - building shouldn't happen on swarm, it should happen by CI tool jenkins and upload to repository
        + docker-compose now ignores 'deploy:' (on local machine), docker swarm ignores 'build:'
        + docker-compose cli not needed on swarm server - it's not a production tool, it's used on local m/c
        + stack is only for one swarm
      + node1% sudo docker service ls; # lists services
      + node1% sudo docker service rm db redis result vote worker; # remove services
      + % cp material/udemy-docker-master/swarm-stack-1 work/. && cd work/swarm-stack-1
      + % scp -r example-voting-app-stack.yml root@node1ip:.
      + node1% sudo docker stack deploy -c example-voting-app-stack.yml voteapp; # deploy's app
      + node1% sudo docker stack --help; # docker stack help
      + node1% sudo docker stack ls; # lists stack's and it's services
      + node1% sudo docker stack ps voteapp; # lists processes
      + node1% sudo docker stack services voteapp; # lists services; this and above command are useful
      + node1% sudo docker network ls; # lists network
      + in browser <node3ip>:5000 for vote and <node2ip>:5001 for results; # vote is on node3 and result on node2
      + in browser <node3ip>:8080 for visualize; # visualizer is on node3; gives details of nodes and it's service details in visual form
      + change vote replicas to 3 in example-voting-app-stack.yml for update
      + node1% sudo docker stack deploy -c example-voting-app-stack.yml voteapp; # updates services
      + resources
        + https://docs.docker.com/compose/compose-file/#not-supported-for-docker-stack-deploy - features not supported in stack deploy
***** secrets storage for swarm: protecting your environment variables
      + secrets storage
        + easiest "secure" solution for storing secrets in swarm
        + secrets are - usernames and passwords; TLS certificates and keys; SSH keys ...
        + supports generic strings or binary content up to 500kb in size
        + doesn't require app to be re-written
        + as of docker 1.13.0 swarm raft DB is encrypted on disk
        + only stored on disk on manager nodes
        + default is managers and workers "control plane" is TLS + Mutual Auth
        + secrets are first stored in swarm db, then assigned to a service(s)
        + only containers is assigned service(s) can see them
        + they look like files in container but are actually in-memory fs
        + /run/secrets/<secret_name> or /run/secrets/<secret_alias>
        + local docker-compose can use file-based secrets, but not secure - fakes secrets, secrets are used to run on swarm
***** using secrets in swarm services
      + % cp material/udemy-docker-mastery/secrets-sample-1 work/. && cd work
      + % scp -r secrets-sample-1 root@node1ip:.
      + node1% cd secrets-sample-1
      + node1% sudo docker secret create psql_user psql_user.txt
      + node1% echo "myDBpassWORD" | sudo docker secret create psql_pass - ; # '-' is read from input; not safe as it's in history
      + node1% sudo docker secret ls; # lists secret
      + node1% sudo docker secret inspect psql_user; # info about psql_user
      + node1% sudo docker service create --name psql --secret psql_user --secret psql_pass -e POSTGRES_PASSWORD_FILE=/run/secrets/psql_pass -e POSTGRES_USER_FILE=/run/secrets/psql_user postgres; # run service using secrets
      + node1% sudo docker service ps psql
      + node1% sudo docker exec -it <containername> bash
      + c% cat /run/secrets/psql_user; # on node3 as psql launched on node3
      + c% cat /run/secrets/psql_pass
      + c% exit
      + resources
        - https://docs.docker.com/engine/swarm/secrets/ - manage sensitive data with docker secrets (docker docs) (lots of good reading and example)
***** using secrets with swarm stacks
      + % cp material/udemy-docker-mastery/secrets-sample-2 work/. && cd work
      + % scp -r secrets-sample-2 root@node1ip:.
      + node1% cd secrets-sample-2
      + node1% sudo docker stack deploy -c docker-compose.yml mydb; # run secrets via stack (compose)
      + node1% sudo docker stack rm mydb; # removes secrets as well
      + resources
        + https://docs.docker.com/compose/compose-file/#secrets-configuration-reference - secrets in compose files (docker docs)
***** using secrets with local docker compose
      + % cd work/secrets-sample-2; # on local machine
      + % sudo docker-compose up -d; # run docker-compose
      + % sudo docker-compose exec psql cat /run/secrets/psql_user.txt; # run exec command on psql through docker-compose; compose not secure but works on local
      + % sudo docker-compose exec psql cat /run/secrets/psql_password.txt; # run exec command on psql through docker-compose; usefully to test locally
***** assignment answers: create a stack with secrets and deploy
      + % cp compose-assignment-2 secrets-assignment-1
      + stack and compose works on docker-compose.yml version '3.1'
      + edit secrets-assignmet-1/docker-compose.yml
      + % scp -r secrets-assignment-2 root@node1ip:.
      + node1% echo "abcxyz" | docker secret create psql-pw -
      + node1% sudo docker stack deploy -c docker-compose.yml drupal
      + node1% sudo docker stack ps 
      + in browser http://<node1ip>:8080
***** full app lifecycle: dev, build and deploy with a single compose design
      + full app lifecycle with compose
        + swarm+stacks+secrets are good features for production
        + local 'docker-compose up' development environment
        + remote 'docker-compose up' CI environment
        + remote 'docker stack deploy' production environment
      + % cp -rf materials/udemy-docker-mastery/swarm-sample-3 work %% cd work/swarm-sample-3
      + 'Dockerfile' - build image
      + 'docker-compose.yml' - called in all docker-compose.*.yml files
      + 'docker-compose.override.yml' - local development, docker-compose automatically reads this and overrides docker-compose.yml; has bind-mount of themes
        + file secret
      + 'docker-compose.prod.yml' - production environment, need to specify file manually; no bind-mount; need to use docker config
        + external secret
      + 'docker-compose.test.yml' - test environment, need to specify file manually; docker-compose -f
        + jenkin CI solution run on commit; no need to define volumes because no need to keep named volumes because as soon as the test passes or fails the data is removed
        + sample_data scenario, in CI solution may be comes from custom data repository. instead of creating database every single time for testing we bind mount sample data 
        + file secret
      + % sudo docker-compose up -d; # deveopment environment: uses docker-compose.yml first then overlay docker-compose.override.yml
      + % sudo docker inspect drupal; # all the mounts installed
      + % sudo docker-compose down
      + % sudo docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d; # test environment; no bind mounts as we didn't specify
      + % sudo docker-compose down
      + % sudo docker-compose -f docker-compose.yml -f docker-compose.prod.yml config > output.yml; # prod environment; output.yml is used in production stack
      + % scp -r output.yml root@<node1ip>:.
      + node1% sudo docker stack deploy -c output.yml drupal
      + Note: compose extends: another way to override file, doesn't work yet in stacks
      + resources
        + https://docs.docker.com/compose/extends/#multiple-compose-files - using multiple compose files (docker docs)
        + https://docs.docker.com/compose/production/ - using compose files in production (docker docs)
**** container registries: image storage and distribution
***** docker hub: digging deeper
      + container registries
        + an image registry needs to be part of your container plan
      + docker hub
        + most popular public image registry
        + docker registry+lightweight image building
        + 'hub.docker.com' # docker hub link
          + one private free
          + "webhooks" - images are build and pushed to docker hub; send webhooks to jenkins CI for auto build for automation
          + "collaborators" - permissions for other users
          + can create organisations which is free
          + if you are using github/bitbucket don't use hub.docker.com create repository
            + use 'create automated build' - allows CI path to github/bitbucket to build images based on code commits (reverse webhooks)
            + tags images with "automated build"
            + go through 'build details' and 'build settings'
            + in 'build settings' for FROM image; add Repository link to that image; so that any change in that image it builds image
            + 'build triggers' - some other software not github to trigger build
      + resources
        + https://hub.docker.com - docker hub
***** docker store: what is it for ?
      + store.docker.com
        + download docker "editions"
        + find certified docker/swarm plugins and commercial certified images
        + no different versions of images
        + 'dockerhub' is 'git hub' of images; 'docker store' is like app store for images
      + resources
        + https://store.docker.com - docker store
***** docker cloud: CI/CD and server ops
      + docker cloud
        + web based Docker Swarm creation/management
        + uses popular cloud hosters (aws, do) and bring-your-own-server and don't want to use cli and manage
        + you can pay for automated image building, testing and deployment - CI/CD platform
        + includes a image security scanning service
        + for official images in docker hub under tags you can see vunerabilities (which are inter run by docker cloud security ???)
      + resources
        + https://cloud.docker.com - docker cloud
***** use docker cloud for easy remote swarm management
      + resources
        + https://www.youtube.com/watch?v=VJmbCioYKGg&feature=youtu.be - fleet management and collaboration
***** understanding docker registry
      + docker regitry
        + image storage and distribution
        + https://github.com/docker/distribution - docker source code
        + https://hub.docker.com/_/registry/ - official docker repository
        + at its core: a web API and storage system, written in Go
        + storage supports local, S3/Azure/Alibaba/Google Cloud and OpenStack Swift
        + look in section resources for links to:
          + secure your registry with TLS
          + storage cleanup via Garbage Collection
          + enable hub caching via "--registry-mirror" - like a proxy to download image once and used by other machines
      + resources
        + https://docs.docker.com/registry/configuration/ - registry configuration docs
        + https://docs.docker.com/registry/garbage-collection/ - registry garbage collection
        + https://docs.docker.com/registry/recipes/mirror/ - use registry as a "mirror" of docker hub
***** run a private docker registry
      + using docker registry local
      + registry and proper TLS
        + "secure by default": docker won't talk to registry without HTTPS
        + except, localhost (127.0.0.0/8)
        + for remote self-signed TLS, enable "insecure-registry" in engine - not recommended
      + % cp material/udemy-docker-mastery/registry-sample-1 work/. && cd work/registry-sample-1
      + % sudo docker container run -d -p 5000:5000 --name registry registry
      + % sudo docker container ls
      + % sudo docker image ls
      + % sudo docker pull hello-world
      + % sudo docker run hello-world
      + % sudo docker tag hello-world 127.0.0.1:5000/hello-world; # own official image at root local registry
      + % sudo docker image ls
      + % sudo docker push 127.0.0.1:5000/hello-world; # pushes to local registry
      + % sudo docker image remove hello-world
      + % sudo docker container rm <containername>
      + % sudo docker remove 127.0.0.1:5000/hello-world; # image removed
      + % sudo docker pull 127.0.0.1:5000/hello-world; # pull's from local registry
      + % sudo docker container kill registry
      + % sudo docker container rm registry
      + % sudo docker container run -d -p 5000:5000 --name registry -v $(pwd)/registry-data:/var/lib/registry registry; # new registry with new database
      + % sudo docker image ls
      + % sudo docker push 127.0.0.1:5000/hello-world
      + % ll registry-data
      + % tree registry-data/ # shows registry
        + "tree" show dirs and files visually
          + linux: apt/yum install tree
          + macOS: brew install tree
          + windows: already installed
        + show data, blob (binaries) and meta data
      + summarize: run a private docker registry recap
        + run the registry image
          + 'docker container run -d -p 5000:5000 --name registry registry'
        + re-tag an existing image and push it to your registry
          + 'docker tag hello-world 127.0.0.1:5000/hello-world'
          + 'docker push 127.0.0.1:5000/hello-world'
        + remove that image from local cache and pull it from new registry
          + 'docker image remove hello-world'
          + 'docker image remove 127.0.0.1:5000/hello-world'
          + 'docker pull 127.0.0.1:5000/hello-world'
        + re-create registry using a bind mount and see how it stores data
          + 'docker container run -d -p 5000:5000 --name registry -v $(pwd)/registry-data:/var/lib/registry registry'
***** assignment: secure docker registry with TLS and authentication
      + default registry installed is rather bare bones, and is open by default, meaning anyone can push and pull images
      + we should at lesat add TLS to it so you can work with it easily via HTTPS and then also add some basic authentication
      + try it on play-with-docker
        + http://training.play-with-docker.com/linux-registry-part2/ - docker registry for linux parts 2 & 3
        + http://training.play-with-docker.com/linux-registry-part1/ - docker registry for linux part 1
        + http://training.play-with-docker.com/ - for more extra credit labs
***** using docker registry with swarm
      + private docker registry with swarm
        + works the same way as localhost
        + because of routing mesh, all nodes can see 127.0.0.1:5000
        + remember to decide how to store images (volume driver)
        + go to play-with-docker.com
          + go to 'setings' and click 5 managers and no workers
          + % docker node ls; # 5 nodes are running
          + % docker service create --name registry --publish 5000:5000 registry
          + % docker service ps regsitry; # registry is running; in browser click the port 5000 and add ....com/v2/_catalog returns a simple json array (as registry is empty)
          + % docker pull hello-world
          + % docker tag hello-world 127.0.0.1:5000/hello-world
          + % docker push 127.0.0.1:5000/hello-world
          + % docker image rm hello-world 127.0.0.1:5000/hello-world; # in browser click the port 5000 and add ....com/v2/_catalog returns a json array with one entry hello-world
          + % docker pull nginx
          + % docker tag nginx 127.0.0.1:5000/nginx
          + % docker push 127.0.0.1:5000/nginx
          + % docker service create --name nginx -p 80:80 --replicas 5 --detach=false 127.0.0.1:5000/nginx; # shows command running in real time
            + because it's run as a service, all node able to push and pull images from local repository
          + % docker service ps nginx; # 1 running one each node and with local image
          + all nodes must be able to access image, routed mesh makes it easy
          + ProTip: use a hosted SaaS registry if possible - try these first and then only local hosted platform because of utility
***** third party image registry
      + image registry
        + docker hub
        + docker enterprise edition DTR (docker trusted registry)
        + docker registry
        + Quay.io - popular choice and comparable to docker hub as cloud-based registry
          + https://sysdig.com/blog/sysdig-docker-usage-report-2017/ - sysdig report on registry usage
        + aws, azure and google cloud have their own registry options well integrated with their toolset
        + self-hosted options
          + Docker EE
          + Quay Enterprise
          + GitLab which comes with GitLab container registry
            + https://about.gitlab.com/2016/05/23/gitlab-container-registry/
        + more list of registries
          + https://github.com/veggiemonk/awesome-docker#hosting-images-registries - list of docker resources
**** bonus section
     + bret's dockercon 2017 video: "Journey to Docker Production"
       + https://www.youtube.com/watch?v=ZdUcKtg84T8
       + https://dockercon.docker.com/watch/WdAeLaLuSCNQwEp61YVXUt
     + swarm quorum and recovery (laura frank from dockercon 2017)
       + https://www.youtube.com/watch?v=Qsv-q8WbIZY
     + bret's docker and devoops newsletter
       + https://www.getdrip.com/forms/54453022/submissions/new - register here
     + using prune to keep your docker system clean
       + https://www.youtube.com/watch?v=_4QzP7uwtvI&feature=youtu.be
     + what's new in docker 17.06
       + https://www.youtube.com/watch?v=-NeaXUGEK_g
       + https://github.com/docker/docker-ce/releases - detailed list of changes
     + Node.js Good defaults for docker
       + https://github.com/BretFisher/node-docker-good-defaults - github repo, go through README.md; best setup a Node app in Docker for local development and production use
** kubernetes
*** udemy - learn devops: the complete kubernetes course - edward viaene
**** course introduction
     + course overview
       + introduction
         + what is kubernetes; cloud/on-premise setup; cluster setup; building containers; running your first app; building container images
       + kubernetes basics
         + node architecture; scaling pods; deployments; services; labels; healthchecks; secrets; WebUI
       + advanced topics
         + service auto-discovery; configmap; ingress; volumes; pet sets; daemon sets; monitoring; autoscaling
       + administration
         + master services; quotas and limits; namespace; user management; networking; node maintenance; high availability
     + course repo
       + https://github.com/wardviaene/kubernetes-course
     + procedure document
       + download kubectl
         + linux - https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubectl
         + macos - https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/darwin/amd64/kubectl
         + windows - https://github.com/eirslett/kubectl-windows/releases/download/v1.6.3/kubectl.exe
       + minikube
         + project url - https://github.com/kubernetes/minikube
         + download instructions - https://github.com/kubernetes/minikube/releases
         + virtual box - http://www.virtualbox.org
         + windows
           + download latest minikube-version.exe
           + rename the file to minikube.exe and put it in C:\minikube
           + open a cmd and run 'cd C:\minikube' and enter 'minikube start'
           + test your commands
             + 'minikube status'
             + 'kubectl run hello-minikube -- image=gcr.io/google_containers/echoserver:1.4 --port=8080 kubectl expose deployment hello-minikube --type=NodePort'
             + 'minikube service hello-minikube --url'
       + kops
         + project url: https://github.com/kubernetes/kops
       + free dns service
         + http://freedns.afraid.org/
         + choose for subdomain hosting
         + enter aws nameservers give to you in route53 as nameservers for the subdomain
         + http://www.dot.tk/ - provides a free.tk domain name you can use and you can point it to the amazon aws nameservers
         + namecheap.com often has promotions for tld's like .co for just a couple of bucks
       + cluster commands
         + 'kops create cluster --name=kubernetes.newtech.academy --state=s3://kops-state-b429b --zones=eu-west-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=kubernetes.newtech.academy'
         + 'kops update cluster kubernetes.newtech.academy --yes --state=s3://kops-state-b429b'
         + 'kops delete cluster --name kubernetes.newtech.academy --state=s3://kops-state-b429b'
         + 'kops delete cluster --name kubernetes.newtech.academy --state=s3://kops-state-b429b --yes'
       + kubernetes from scratch
         + you can setup your cluster manually from scratch
         + if you're planning to deploy on AWS/Google/Azure, use the tools that are fit for these platforms
         + if you have an unsupported cloud platform, and you still want kubernets, you can install it manually
         + coreOS + kubernetes
           + https://coreos.com/kubernetes/docs/latest/getting-started.html
       + docker
         + download docker engine
           + windows:  https://docs.docker.com/engine/installation/windows/
           + macos: https://docs.docker.com/engine/installation/mac/
           + linux: https://docs.docker.com/engine/installation/linux/
       + devops box
         + virtualbox: http://www.virtualbox.org
         + vagrant: http://www.vagrantup.com
         + devops box: https://github.com/wardviaene/devops-box
           + vagrant project with an ubuntu box with the tools needed to do DevOps
           + tools included: Terraform, aws cli, ansible
         + launch commands (in terminal)
           + 'cd devops-box && vagrant up'
         + launch commands for a plain ubuntu
           + 'mkdir ubuntu'
           + 'vagrant init ubuntu/xenial64'
           + 'vagrant up'
       + docker commands
         + 'docker build' - build image
         + 'docker build -t wardviaene/k8s-demo:latest .' - build & tag
         + 'docker tag imageid wardviaene/k8s-demo' - tag image
         + 'docker push wardviaene/k8s-demo' - push image
         + 'docker images' - list images
         + 'docker ps -a' - list all containers
       + kubernetes commands
         + 'kubectl get pod' - get info about all running pods
         + 'kubectl describe pod <pod>' - describe one pod
         + 'kubectl expose pod <pod> --port=444 --name=frontend' - expose the port of a pod (creates a new service)
         + 'kubectl port-forward <pod> 8080' - port forward the exposed pod port to your local machine
         + 'kubectl attach <podname> -i' - attach to the pod
         + 'kube exec <pod> --command' - execute a command on the pod
         + 'kubectl label pods <pod> mylabel=awesome' - add a new label to a pod
         + 'kubectl run -i --tty busybox --image=busybox --restart=Never -- sh' - run a shell in a pod - very useful for debugging
         + 'kubectl get deployments' - get information on current deployments
         + 'kubectl get rs' - get information about the replica sets
         + 'kubectl get pods --show-labels' - get pods, and also show labels attached to those pods
         + 'kubectl rollout status deployment/helloworld-deployment' - get info on current deployments
         + 'kubectl get rs' - get info about the replica sets
         + 'kubectl get pods --show-labels' - get pods, and also show labels attached to those pods
         + 'kubectl rollout status deployment/helloworld-deployment' - get deployment status
         + 'kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo-2' - run k8s-demo with the image label vesion 2
         + 'kubectl edit deployment/helloworld-deployment' - edit the deployment object
         + 'kubectl rollout history deployment/helloworld-deployment' - get the rollout history
         + 'kubectl rollout undo deployment/helloworld-deployment' - rollback to previous version
         + 'kubectl rollout undo deployment/helloworld-deployment --to-version=n' - rollback to any version
       + aws commands
         + 'aws ec2 create-volume --size 10 --region us-east-1 --availability-zone us-east-1a --volume-type gp2'
       + certificates
         + 'openssl genrsa -out myuser.pem 2048' - creating a new key for a new user
         + 'openssl req -new -key myuser.pem -out myuser -csr.pem -subj "/CN=myuser/O=myteam/" - creating a certificate request
         + 'openssl x509 -req -in myuser-csr.pem -CA /path/to/kubernetes/ca.crt -CAkey /path/to/kubernetes/ca.key - CAcreateserial -out myuser.crt -days 10000' - creating a certificate
**** introduction to kubernetes
***** kubernetes introduction
      + kubernetes is an open source orchestration system for docker containers
        + schedules containers on a cluster of machines
        + can run multiple containers on one machine
        + can start the container on specific nodes
        + can move containers from one node to another node
        + clusters can start with one node until thousands of nodes
        + other docker orchestrators: docker swarm (not as extensive as kubernetes), mesos
        + can run on-premis (own datacenter), public (aws, gcp), hybrid (public & private)
        + highly moduler
        + open source and available on github
        + originally developed by google called borg and released to open source
***** containers introduction
      + vm: has a guest OS on host OS with hypervisor (phy server->hostOS->Hypervisor->GuestOS->bins/libs->apps); need full boot cycle
      + container: no guest OS; no boot cycle (phy server->hostOS->bins/libs->app with Docker Engine); no boot cycle and runs as process
        + but container on cloud provider still has Hypervisor and GuestOS but still gets all benefits of containers
        + Docker is most popular container software
          + rkt - another container software works with kubernetes as well
        + Docker Engine is Docker runtime to make run docker images
        + Isolation: you ship a binary with all the dependencies
        + You can run the same docker image, unchanged, on laptops, data center VMs and Cloud providers
        + Docker uses Linux Containers (a kernel feature) for operating system-level isolation
***** kubernetes setup
      + things like volumes and external load balancers work only with supported cloud providers
        + for aws, gcp and azure it's available
      + 'minikube' to quickly spin up a local single machine with a kubernetes cluster
      + 'kops' to spin up a highly available production cluster on aws (http://aws.amazon.com)
      + https://github.com/kubernetes/minikube - minikube github
      + can use digital ocean (https://www.digitalocean.com)
***** local setup with minikube
      + 'minikube' is a tool that makes it easy to run kubernetes locally
        + runs a single-node kubernetes cluster inside a linux vm
        + aimed on users to just test it out or use it for development
        + cannot spin up a production cluster, it's one node machine with no high availability
        + works on windows, linux and macos and need virtualization software
          + virtualbox - www.virtualbox.org
        + 'minikube start' - to start a cluster
***** demo: minikube
      + go to "https://github.com/kubernetes/minikube/releasese"
      + minikube install
        + macos:
          + % brew cask install minikube 
          + or % brew cask reinstall minikube; # to upgrade minikube
          + or % curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
        + linux:
          % curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
        + % minikube; # lists commands
      + kubectl install
        + macos:
          + % curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl
          + % chmod +x kubectl
          + % mv kubectl /usr/local/bin/kubectl
        + linux:
          + % curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
          + % chmod +x kubectl
          + % mv kubectl /usr/local/bin/kubectl
          + % kubectl; # lists command
      + virtual box install
        + ubuntu xenial install
          + add official virtualbox repository
            + % echo "deb http://download.virtualbox.org/virtualbox/debian $(lsb_release -cs) contrib" | sudo tee /etc/apt/sources.list.d/oracle-virtualbox.list
          + download and add keys
            + % wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -
            + % wget -q https://www.virtualbox.org/download/oracle_vbox.asc -O- | sudo apt-key add -
          + install virtualbox
            + % sudo apt-get update
            + % sudo apt-get install virtualbox-5.2
      + % minikube start; # start minikube
        + ~/.kube/config - kubectl config file created; kubectl will use this config
      + % kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080; # name:hello-minikube image from google echo server
        + % kubectl delete service,deployment hello-minikube; # to delete earlier service and deployment
        + % kubectl get pods; # get pods list
        + % kubectl get services; # get services list
      + % kubectl expose deployment hello-minikube --type=NodePort; # expose port to host machine
      + % minikube service hello-world --url; # download's image and gives url
      + go to browser and checker 'url'

          
**** kubernetes basics
**** advanced topics
**** kubernetes administration
**** course completion
    
