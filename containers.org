* containers notes
** docker
*** udemy - docker mastery: the complete toolset from a docker captain - bret fisher
**** course introduction and docker setup
***** getting course resources
      + resources
        + https://github.com/bretfisher/udemy-docker-mastery - course repository
***** installing docker: the fast way
      + installation
        + ubuntu:xenial64
 	  + install docker-ce
  	    + % sudo apt-get remove docker docker-engine docker.io; # remove old versions
            + % sudo apt-get update; # update the apt package index
	    + % sudo apt-get install apt-transport-https ca-certificates curl software-properties-common; # install packages to allow apt to use repository over HTTPS
	    + % curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -; # add docker's official GPG key
	    + % sudo apt-key fingerprint 0EBFCD88; # verify key with the fingerprint
	    + % sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"; # set up stable repository
	    + % sudo apt-get update; # update the apt package index
	    + % sudo apt-get install docker-ce; # install latest version of docker ce
	    + % sudo docker run hello-world
	    + to avoid typing sudo (not recommended)
	      + % sudo usermod -aG docker <user>; # add <user> to docker group
	      + relogin
          + install docker-compose
	    + % sudo curl -L https://github.com/docker/compose/releases/download/1.17.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose; # download and install docker compose
	    + % sudo chmod +x /usr/local/bin/docker-compose
	    + % docker-compose --version
	    + % sudo curl -L https://raw.githubusercontent.com/docker/compose/1.17.0/contrib/completion/bash/docker-compose -o /etc/bash_completion.d/docker-compose; # Place following script in /etc/bash_completion.d/
	    + re-login
          + install docker-machine
	    + % curl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` > /tmp/docker-machine ; # downlaod docker machine and extract to your PATH
            + % sudo chmod +x /tmp/docker-machine
            + % sudo cp /tmp/docker-machine /usr/local/bin/docker-machine
	    + % docker-machine version; # check docker machine
	    + % scripts=( docker-machine-prompt.bash docker-machine-wrapper.bash docker-machine.bash ); for i in "${scripts[@]}"; do sudo wget https://raw.githubusercontent.com/docker/machine/v0.13.0/contrib/completion/bash/${i} -P /etc/bash_completion.d; done
	    + re-login
	  + because docker-compose and docker-machine are installed manually, make sure you check for latest version frequently
      + run more nodes: % docker-machine create --driver
        + virtualbox, aws etc. https://docs.docker.com/machine/drivers/
      + resources
        + https://labs.play-with-docker.com - run one or more docker instances inside your browser
        + https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/ - ubuntu docker installation
        + https://get.docker.com; # install docker via running script
	  + % curl -fsSL get.docker.com -o get-docker.sh
	  + % sh get-docker.sh
        + https://docs.docker.com/machine/install-machine/ - docker machine install (docker docs)
        + https://docs.docker.com/compose/install/ - docker compose install (docker docs)
        + www.bretfisher.com/shell - customize shell      
***** docker for windows 10 pro/ent: setup and tips
      + resources
        + https://store.docker.com/editions/community/docker-ce-desktop-windows - download docker ce for windows
        + https://desktop.github.com - download github desktop
        + https://code.visualstudio.com - download visual studio code
        + https://docs.docker.com/docker-for-windows/#explore-the-application-and-run-examples - setup tab completion for powershell
        + http://cmder.net - download cmder
        + https://docs.docker.com/docker-for-windows/faqs/ - docker for windows FAQ
***** docker for mac setup and tips
      + resources
        + https://store.docker.com/editions/community/docker-ce-desktop-mac  - docker ce for mac
        + https://www.iterm2.com - iterm2
        + https://docs.docker.com/docker-for-mac/#installing-bash-completion - installing bash completion on mac
        + https://brew.sh - installing homebrew (the brew cli)
***** docker for linux setup and tips
      + resources
        + https://docs.docker.com/compose/install/ - install docker compose 
        + https://docs.docker.com/machine/install-machine/ - install docker machine
**** creating and using containers like a boss
***** check our docker install and config
      + % sudo docker version; # gives version of client and server
      + % sudo docker info; # gives config and setup
      + % sudo docker; # gives list of commands and management commands
      + new syntax "sudo docker <management-commands> <sub-commands>"
***** starting a Nginx web server
      + hub.docker.com - default image "registry"
      + % sudo docker container run --publish 80:80 nginx; # run nginx container
	+ go to "localhost" in browser
	+ "--publish" opened port 80 on the host IP and routes that traffic to the container IP, port 80
	+ % sudo docker container run --publish 80:80 --detach nginx; # to run in background
      + % sudo docker container ls; # lists running containers
      + % sudo docker container stop <container_id>; # stop container
      + % sudo docker container ls -a; # 
      + % sudo docker container run --publish 80:80 --detach --name webhost nginx; # assing container name
      + % sudo docker container logs webhost; # shows logs
      + % sudo docker container top webhost; # lists process running inside container
      + % sudo docker container --help; # list of container commands
      + % sudo docker container rm <container id1> <container id2>; # removes stopped containers
      + % sudo docker container rm -f <container id1> <container id2>; # removes running and stopped containers
***** debrief: what happens when we run a container
      + % sudo docker container run --publish 8080:80 --name webhost -d nginx:1.11 nginx -T; # run nginx version 1.11 and command "nginx -T"
***** container vs vm: it's just a process
      + % sudo docker run --name mongo -d mongo; # run mongodb
      + % sudo docker ps; # lists running containers
      + % ps aux; # lists all running hosts process
      + % ps aux | grep mongo; # filter running mongo process
      + % sudo docker start mongo; # start mongo container
      + resources
        + https://www.youtube.com/watch?v=sK5i-N34im8&feature=youtu.be&list=PLBmVKD7o3L8v7Kl_XXh3KaJl9Qw2lyuFl - docker internals
***** assignment answers: manage multiple containers
      + docs.docker.com and --help are your friend
      + % sudo docker container run -d -p 3306:3306 --name db -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql; # run mysql db, --e is to pass environment variables
      + % sudo docker container logs db; # check for "GENERATED ROOT PASSWORD" from mysql logs
      + % sudo docker container run -d --name webserver -p 8080:80 httpd; # run apache webserver
      + % sudo docker ps; # checker item PORTS for each container
      + % sudo docker container run -d --name proxy -p 80:80 nginx; # run nginx to act as proxy
      + % sudo docker container ls; # same as "sudo docker ps"
      + % curl localhost; # should give nginx
      + % curl localhost:8080; # should give apache
      + % sudo docker container stop <proxy cont id> <webserver cont id> <db cont id>; # you can check tab completion after "sudo docker container stop <tab>"
      + % sudo docker ps -a
      + % sudo docker container rm <proxy cont id> <webserver cont id> <db cont id>
      + % sudo docker ps -a; # everything is cleaned up
      + % sudo docker image ls; # images are still present
***** what's going on in containers: CLI process monitoring
      + % sudo docker container run -d --name nginx nginex; # run nginx
      + % sudo docker run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql; # run mysql
      + % sudo docker container ls
      + % sudo docker container top mysql; # process list in one container
      + % sudo docker container top nginx; # process list in one container
      + % sudo docker container inspect mysql; # details of one container config; shows metadata about the container (startup config, volumes, networking etc)
      + % sudo docker container stats --help; # displays help
      + % sudo docker container stats; # show live performance data for all containers
***** getting a shell inside containers: no need for ssh
      + % sudo docker container run -it --name proxy nginx bash; # start new container interactively, no ssh needed; '-t'='--tty' -> simulates a real terminal like ssh; '-i'='--interactive' -> keep session open to receive terminal input; bash shell if run with -it, it will give you a terminal inside the running container
        + we can do additional things from shell like installing additional packages and etc; '% exit' to exit from shell and container stops
      + % sudo docker container run -it --name ubuntu ubuntu; # full distribution of linux
        + c% apt-get update; # update packages inside ubuntu container
        + c% apt-get install -y curl; # install curl
        + c% curl www.google.com; # running curl inside container
        + c% exit; exit and stops container
      + % sudo docker start -ai ubuntu; # start ubuntu and curl is installed; new run container will not have curl installed; '-a'='--attach', 'i'='--interactive'
      + % sudo docker start --help; #  start command help
      + % sudo docker exec --help; #  exec command help
      + % sudo docker exec -it mysql bash; # run additional command in existing container; bash shell on running container mysql; 
        c% ps aux; # process list inside container
        c% exit; # exit container
      + 'alpine' is another distribution of linux which is a small security focused distribution
      + sudo docker pull alpine; # pull alpine image, has it's own package manager
      + sudo docker container run -it alpine bash; # throws error as bash is not installed
      + sudo docker container run -it alpine sh; # but has shell; package manager is 'apk' which can be used to install bash
      + resources
        + https://www.digitalocean.com/community/tutorials/package-management-basics-apt-yum-dnf-pkg - package management basics:apt,yum,dnf,pkg
***** docker networks: concepts for private and public comms in containers
      + docker networks defaults
        + each container connected to a private virtual network 'bridge'
        + each virtual network routes through NAT firewall on host IP; NAT firewall is docker daemon configuring host IP on its default i/f so that your containers can access internet 
        + all containers on a virtual network can talk to each other without -p
        + best practice is to create a new virtual network for each app
          + network "my_web_app" for mysql and php/apache containers
          + network "my_api" for mongo and nodejs containers
        + defaults work well in many cases, but easy to swap out parts to customize it
        + make new virtual networks and attach containers to more than one virtual network
        + skip virtual networks and use host IP (--net=host)
        + use different docker network drivers to gain new abilities
      + 'docker container run -p' - exposes ports on your machine
      + % sudo docker container run -p 80:80 --name webhost -d nginx; # -p (--publish) publishing port in HOST:CONTAINER format
      + % sudo docker container port webhost; # displays port
      + % sudo docker container inspect --format '{{ .NetworkSettings.IPAddresss }}' webhost; # --format - a common option for formatting the output of commands using "Go templates"; display container ip address which is different that host IP
      + % ifconfig en0; # mac ip address
      + resources
        + https://docs.docker.com/engine/admin/formatting/ - docker's --format for filtering cli output
***** docker networks: cli management of virtual networks
      + a recent june 2017 change, removes ping in nginx; so replace "nginx" to "nginx:latest" which still has ping
      + running docker inside docker (mac)
        + % docker run -v /var/run/docker.sock:/var/run/docker.sock -it ubuntu bash; # run ubuntu container sharing that sock
        + c% apt-get update && apt-get install curl
        + c% curl -sSL https://get.docker.com/ | sh
        + c% docker version
        + c% docker container run hello-world
        + c% exit
      + % sudo docker network ls; # lists all created networks: bridge, host, null; '--network bridge' is default docker virtual network which is NAT'ed behind the HOST IP; '--network host' is gains performance by skipping virtual networks but sacrifices security of container model; '--network none' removes eth0 and only leaves you with localhost interface in container
      + % sudo docker network inspect bridge; # can see list of containers attached to it; IPAM shows default IP's assigned
      + % sudo docker network create my_app_net; # spawns a new virtual network for you to attach containers to
      + % sudo docker network ls; # shows list of networks
      + % sudo docker network --help; # network command help
      + % sudo docker container run -d --name new_nginx --network my_app_net nginx
      + % sudo docker network inspect my_app_net; # new_nginx is on that network
      + % sudo docker network connect <network id1> <network id2>; # connect dynamically creates a NIC in a container on an existing virtual network
      + % sudo docker network inspect <network id1>; # you can see both networks
      + % sudo docker network disconnect <network id1> <network id2>; # disconnect dynamically rmeoves a NIC from a container on a specific virtual network
      + create your apps so frontend/backend sit on same docker network and their inter-communication never leaves host. all externally exposed ports closed by default and must manually expose via -p which is better default security
***** docker networks: dns and how containers find each other
      + static IP's and using IP's for talking to containers is an anti-patern. do your best to avoid it
      + docker daemon has a built in DNS server that containers use by default
      + % sudo docker container ls
      + % sudo docker network inspect <my_app_net network id>; # not default network brigde
      + dns default names, docker defaults the hostname to the container's name, but you can also set aliases
      + % sudo docker container run -d --name my_nginx --network my_app_net; # my_app_net contains two containers
      + % sudo docker container exec -it my_nginx ping new_nginx; # ping new_nginx from my_nginx
      + % sudo docker container exec -it new_nginx ping my_nginx; # ping my_nginx from new_nginx
      + % sudo docker network ls; # default bridge doesn't have default dns server, need to use --link, it's easier to create new network than bridge -l; docker compose makes it easier for dns
***** assignment answers: using containers for cli testing
      + % sudo docker container run --rm -it centos:7 bash
        + c% yum update curl
        + c% curl --version
        + c% exit
      + % sudo docker container run --rm -it ubuntu:14.04 bash
        + c% apt-get update && apt-get install -y curl
        + c% curl --version
        + c% exit
      + % sudo docker ps -a; # both centos and ubuntu disappears
***** assignment answers: dns round robin test
      + From docker engine 1.11, we can have multiple containers on a created network respond to the same DNS address
      + 'elasticsearch' is becoming popular, is RESTful search and analytics engine
      + % sudo docker network create dude
      + % sudo docker container run -d --net dude --net-alias search elasticsearch:2
      + % sudo docker container run -d --net dude --net-alias search elasticsearch:2
      + % sudo docker container ls
      + % sudo docker container run --rm --net dude alpine nslookup search; # shows two dns entries
      + % sudo docker container run --rm --net dude centos curl -s search:9200; # gives first container
      + % sudo docker container run --rm --net dude centos curl -s search:9200; # gives second container in random fashion
      + % sudo docker container ls
      + % sudo docker container rm <container 1> <container 2>
**** container images, where to find them and how to build them
***** what's an image (and what isn't)
      + resources
        + https://github.com/moby/moby/blob/master/image/spec/v1.md - official docker image specification
***** the mighty hub: using docker hub registry images
      + 'https://hub.docker.com' # docker hub
        + official images are great way to start
      + % sudo docker pull nginx; # pulls nginx latest version
      + % sudo docker pull nginx:1.11.9; # pulls nginx 1.11.9 version, especially needed for production development
      + % sudo docker pull nginx:1.11.9-alpine; # 'alpine' is linux distribution which is very small
      + resources
        + https://github.com/docker-library/official-images/tree/master/library - list of official docker images
***** images and their layers: discover the image cache
      + % sudo docker image history nginx:latest; # history show layers of changes made in image
      + % sudo docker image inspect nginx; # inspect returns JSON metadata about the image; gives details of image
      + images are made up of file system changes and metadata; a container is just a single read/write layer on top of image
      + resources
        + https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/ - images and containers from docker docs
***** image tagging and pushing to docker hub
      + % sudo docker image tag --help; # tag assign one or more tags to an image; images are referred as <user>/<repo>:<tag>
      + office repositories live at the "root namespace" of the registry, so they don't need account name in front of repo name
      + % sudo docker pull mysql/mysql-server; # pulls mysql/mysql-server image
      + % sudo docker image tag nginx aneelraju/nginx; # rename image 'nginx' to 'aneelraju/nginx; syntax 'sudo docker image tag 'SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]''
      + % sudo docker login; # 'sudo docker login <server>' defaults to logging in hub, but you can override by adding server url
        + type username and password; # wrote to file ~/.docker/config.json
      + % sudo docker image push aneelraju/nginx; # push uploads changed layers to a image registry (default is hub)
      + % sudo docker image tag aneelraju/nginx aneelraju/nginx:testing
      + % sudo docker image push aneelraju/nginx:testing
      + % sudo docker logout; # always logout from shared machines or servers when done, to protect your account
***** building images: the dockerfile basics
      + 'sudo docker image build -f some-dockerfile'; # builds docker image
      + package manager like apt and yum are one of the reasons to build containers FROM Debian, Ubuntu, Fedora and CentOS
      + environment variables are one reason they were chosen as preferred way to inject key/value is they work everywhere
      + % cp material/udemy-docker-mastery/dockerfile-sample-1 work/. && cd work/dockerfile-sample-1
      + % sudo docker image build -t customnginx .; # builds docker image in current directory
***** building images: extending official images
      + % sudo docker container run -p 80:80 --rm nginx; # default behavior
      + % cp material/udemy-docker-mastery/dockerfile-sample-2 work/. && cd work/dockerfile-sample-2
      + % sudo docker image build -t nginx-with-html .; # as much as possible use popular docker images from docker hub
***** assignment answers: build your own image
      + Dockerfiles are part process workflow and part art
      + % cp material/udemy-docker-master/dockerfile-assignment-1 work/. && cd work/dockerfile-assignment-1
      + prepare dockerfile
      + 'CMD ["executable", "param1", "param2"]'
      + % sudo docker image build -t testnode .
      + % sudo docker container run --rm -p 80:3000 testnode
      + % sudo docker image tag testnode aneelraju/testing-node
      + % sudo docker push aneelraju/testing-node
      + % sudo docker image ls
      + % sudo docker image rm aneelraju/testing-node
      + % sudo docker container run --rm -p 80:3000 aneelraju/testing-node; # downloads from docker hub
**** container lifetime & persistent data: volumes, volumes, volumes
***** container lifetime & persistent data
      + container are usually immutable and ephemeral; "immutable infrastructure": only re-deploy containers, never change
      + container should not mix data with binaries; docker gives us features to ensure these "separation of concerns"
      + for "persistent data" two solutions are available: volumes and bind mounts
      + volumes: make special location outside of container UFS (union file system); container see as a local file path
      + bind mounts: link container path to host path
      + resources
        + https://oreilly.janrainsso.com/static/server.html?origin=https%3A%2F%2Fwww.oreilly.com%2Fideas%2Fan-introduction-to-immutable-infrastructure - intro to immutable infrastructure concepts
        + https://12factor.net - the 12-factor app (everyone should read: key to cloud native app design, deployment and operation)
        + https://medium.com/@kelseyhightower/12-fractured-apps-1080c73d481c - 12 fractured apps (a follow-up to 12-factor, a greate article on how to do 12F correctly in containers)
***** persistent data: data volumes
      + VOLUME command in Dockerfile; # search Dockefile for official mysql in docker file (best way to learn dockerfile best practices); creates and assign volumes; volumes needs manual deletion
      + % sudo docker pull mysql
      + % sudo docker image inspect mysql; # under Config check for "Volumes"
      + % sudo docker container run -d --name mysql -e MYSQL_ALLOW_EMTPY_PASSWORD=True mysql
      + % sudo docker container inspect mysql; # inspect for Mounts and Volumes
      + % sudo docker volume ls; # shows one volume
      + % sudo docker volume inspect <volume name>
      + % sudo docker container run -d --name mysql2 -e MYSQL_ALLOW_EMTPY_PASSWORD=True mysql
      + % sudo docker volume ls; # shows two volumes
      + % sudo docker container stop mysql
      + % sudo docker container stop mysql2
      + % sudo docker container ls
      + % sudo docker container ls -a
      + % sudo docker volume ls; # volumes still present
      + % sudo docker container rm mysql mysql2
      + % sudo docker volume ls; # volumes still present; data still safe
      + 'named volumes' are friendly way to assing vols to containers
      + % sudo docker container run -d --name mysql -e MYSQL_ALLOW_EMTPY_PASSWORD=True -v mysql-db:/var/lib/mysql mysql; # mysql-db is named volume
      + % sudo docker volume ls
      + % sudo docker volume inspect mysql-db
      + % sudo docker container rm -f mysql; # remove container
      + % sudo docker container run -d --name mysql3 -e MYSQL_ALLOW_EMTPY_PASSWORD=True -v mysql-db:/var/lib/mysql mysql
      + % sudo docker volume ls; # doesn't create new volume
      + % sudo docker container inspect mysql3; # name friendly
      + % sudo docker volume create --help; # 'docker volume create' required to do this before "docker run" to use custom drivers and labels
      + sometime you need to create volumes ahead of run, usually for local development specifying in Dockerfile or in run command is fine
***** persistent data: bind mounting
      + mapping a host file or directory to a container file or directory; basically just two locations pointing to the same files
      + can't use in Dockerfile, must be at container run as bind mount are host specific
      + bind mounts starts with '/' and full path
      + % cd work/dockerfile-sample-2
      + % sudo docker container run -d --name nginx -p 80:80 -v $(pwd):/usr/share/nginx/html nginx; # bind mounts current directory
      + % sudo docker container exec -it nginx bash; # get shell into container
      + c% cd /usr/share/nginx/html; # we can host files 
***** assignment answers: database upgrades with named volumes
      + get volume path from docker hub postgres:9.6.1
      + % sudo docker container run -d --name psql -v psql:/var/lib/postgresql/data postgres:9.6.1
      + % sudo docker container logs -f psql; # -f keeps watching as it runs
      + ctrl-c
      + % sudo docker container stop <container id>
      + % sudo docker container run -d --name psql2 -v psql:/var/lib/postgresql/data postgres:9.6.2
      + % sudo docker container ps -a
      + % sudo docker volume ls; # one volume psql
      + % sudo docker container logs <container id>; # postgress is successfully upgraded
***** assignment answers: edit code running in containers with bind mounts
      + good for local development and local testing (similar to vagrant /vagrant folder); allows you to edit files on host machine
      + "Jekyll" is static site generator; # for a simple web site
      + % cp material/udemy-docker-mastery/bindmount-sample-1 work/. && cd work/bindmount-sample-1
      + % sudo docker run -p 80:4000 -v $(pwd):/site bretfisher/jekyll-serve
      + resources
        + https://jekyllrb.com - jekyll, a static site generator
        + https://hub.docker.com/r/bretfisher/jekyll-serve/ - info about how to use jekyll in a docker container for easy SSG development
**** making it easier with docker compose: the multi-container tools
***** docker compose and the docker-compose.yml file
      + docker componse is a cli tool and configuration file; it is comprised of 2 separate but related things
        + 1. YAML formatted file that describes our solution options for: containers, networks and volumes
        + 2. A cli tool docker-compose used for local dev/test automation with those YAML files
      + % sudo docker-compose --help; # docker compose help
      + % cp material/udemy-docker-mastery/compose-sample-1 work/. && cd work/compose-sample-1
      + go through template.yml, docker-compose.yml, compose-2.yml, compose-3.yml
      + resources
        + http://www.yaml.org/start.html - the yaml format: sample generic yaml file
        + http://www.yaml.org/refcard.html - the yaml format: quick reference
        + https://docs.docker.com/compose/compose-file - docker compose file
        + https://docs.docker.com/compose/compose-file/compose-versioning/ - compose file version differences (docker docs)
        + https://github.com/docker/compose/releases - docker compose release downloads (good for linux users that need to download manually)
        + https://wordpress.com - website using wordpress
        + https://ghost.org - open source professional publishing platform
***** trying out basic compose commands
      + docker-compose cli is not a production-grade tool but idea for local development and test
      + 'docker-compose up' - setup volumes/networks and start all containers
      + 'docker-compose down' - stop all containers and remove cont/vol/net
      + if your projects had a Dockerfile and docker-compose.yml then "new developer onboarding" would be
        + 'git clone github.com/some/software'
        + 'docker-compose up'
      + % cp material/udemy-docker-mastery/compose-sample-2 work/. && cd work/compose-sample-2
      + % sudo docker-compose up; # docker-compose is not installed by default on linux with docker
        + % ctrl-c
      + % sudo docker-compose -d; # to run it in the background
      + % sudo docker-compose logs; # to see logs
      + % sudo docker-compose --help; # docker-compose help
      + % sudo docker-compose ps; # list of running containers
      + % sudo docker-compose top; # streaming running process
      + % sudo docker-compose down; # stop and clean-up running processes
      + resources
        + https://github.com/docker/compose/releases - docker-compose download for linux via github, win/mac already have it
***** assignment answers: build a compose file for a multi-container service
      + 'drupal' is content management system website
      + % cp material/udemy-docker-mastery/compose-assignment-2 work/. && cd work/compose-assignment-2
      + % sudo docker pull drupal
      + % sudo docker image inspect drupal
      + check for exposed ports
      + compose docker-compose.yml file
      + % sudo docker-compose up
      + % sudo docker-compose down -v; # remove volumes as well
      + resources
        + https://www.drupal.org; # opensource content management framework
***** adding image building to compose files
      + % cp material/udemy-docker-mastery/compose-sample-3 work/. && cd work/compose-sample-3
      + % sudo docker-compose up
      + % sudo docker-compose down -rmi local; # remove volumes and images as well
      + resources
        + https://docs.docker.com/compose/compose-file/#build - (docker docs) compose file build options
***** assignment answers: compose for run-time image building and multi-container dev
      + % cp work/compose-assignment-2 work/compose-assignment-3 && cd work/compose-assignment-3
      + edit docker-compose.yml and Dockerfile according to README.md
      + % docker-compose up -d
      + configure drupal and install bootstrap theme
      + % docker-compose down
      + % docker-compose up; # website starts directly without installation process
**** docker services and the power of swarm: built-in orchestration
***** swarm mode: built-in orchestration
      + swarm mode is a clustering solution built inside docker
      + swarm commands (docker swarm/node/service/stack/secret) aren't enabled by default
      + docker service (replaces docker run command on swarm)
        + manager node:
          + RAFT
            + API - Accepts command from client and creates service object
            + Orchestrator - Reconcillation loop for service objects and creates tasks
            + Allocator - Allocates IP addresses to tasks
            + Scheduler - Assigns nodes to tasks
            + Dispatcher - Checks in on workers
        + Worker Node
          + Worker - Connects to dispatcher to check on assigned tasks
          + Executor - Executes the tasks assigned to worker node
      + resources
        + https://www.youtube.com/watch?v=dooPhkXT9yI - docker 1.12 swarm mode deep dive part 1: topology (YouTube)
        + https://www.youtube.com/watch?v=dooPhkXT9yI - docker 1.12 swarm mode deep dive part 2: topology (YouTube)
        + https://speakerdeck.com/aluzzardi/heart-of-the-swarmkit-topology-management - Heart of the SwarmKit: Topology Management (slides)
        + https://www.youtube.com/watch?v=EmePhjGnCXY - Heart of the SwarmKit: Store, Topology & Object Model (YouTube)
        + http://thesecretlivesofdata.com/raft/ - Raft Consensus Visualization (Our Swarm DB and how it stays in sync across nodes)
***** create your first service and scale it locally
      + % sudo docker info; # Displays systemwide info; check for Swarm: inactive
      + % sudo docker swarm init; # create a single node cluster on localhost and make current node as manager
        + lots of PKI and security automation
          + root signing certificate created for our swarm
          + certificate is issued for first manager node
          + join tokens are created
        + raft database created to store root CA, configs and secrets
          + encrypted by default on disk (1.13+)
          + no need for another key/value system to hold orchestration/secrets
          + replicates logs amongst managers via mutual TLS in "control plane"
          + raft ensures consistency esp for cloud deployment
      + % sudo docker node ls; # lists nodes
      + % sudo docker node --help; # docker node help
      + % sudo docker swarm --help; # docker swarm help
      + % sudo docker service --help; # docker service help; # 'docker run' is designed for single cluster and 'docker service' is for multi-node clusters
      + % sudo docker service create alpine ping 8.8.8.8; # gives service id not container id
      + % sudo docker service ls; # lists services
      + % sudo docker service ps <servicename>
      + % sudo docker service update <serviceid> --replicas 3; # scaling up services
      + % sudo docker service ls
      + % sudo docker service ps; # 3 services are run
      + % sudo docker update --help; # docker update help; update configuration of one or more containers
      + % sudo docker service update --help; # update a service without taking service down
      + % sudo docker container rm -f <containerid>; # container is removed but it is removed from behind, service will launch new service
      + % sudo docker service rm <servicename>; # removes containers through service
      + % sudo docker service ls; # service is removed
      + % sudo docker container ls; # containers are removed
      + resources
        + https://docs.docker.com/engine/swarm/services/ - deploy services to a swarm (docker docs)
***** creating a 3-node swarm cluster
      + 'play-with-docker.com'; # only needs a browser, but resets after 4 hours
        + quickly create nodes (3 nodes) and each nodes can talk to each other
        + useful if have slower machine and try quickly online
        + runs docker inside docker and has latest tools installed
      + docker-machine + virtualbox; # free and runs locally, but requires a machine with 8GB memory
        + % sudo docker-machine --help; # docker-machine help
        + % sudo docker-machine create node1; # create node through virtual busy box (light weight linux machine)
        + % sudo docker-machine ssh node1; # ssh to node1
          + or % sudo docker-machine env node1; # displays env
          + % eval $(docker-machine env node1); # copy on command prompt
          + docker terminal commands on node1
      + roll you own
        + docker-machine can provision machines for amazon, azure, do, google etc
        + install docker anywhere with get.docker.com; # preferred method as docker-machine may not be used in production settings
      + digital ocean + docker install
        + cheapest and easiest to start digital service
        + most like a production setup, but costs $5-10/node/month
        + create 3 droplets (node1,node2,node3) on digital ocean with 1vCPU + 1GB standard machine ($10/node/month)
        + % ssh root@<node1ip>
        + node1% curl -sSL https://get.docker.com/ | sh; # install docker
        + % ssh root@<node2ip>
        + node2% curl -sSL https://get.docker.com/ | sh; # install docker
        + % ssh root@<node3ip>
        + node3% curl -sSL https://get.docker.com/ | sh; # install docker
        + node1% sudo docker swarm init --advertise-addr <node1 ip>; # docker swarm init
        + node2% sudo docker swarm join --token SWMTKN-1-2th56plepacrp8d8giiyqrtv86m803ia7kiecrzog9xhd9z26h-1mhl4b3x04l2vutj87olww5cn <node1ip>:2377
        + node3% sudo docker swarm join --token SWMTKN-1-2th56plepacrp8d8giiyqrtv86m803ia7kiecrzog9xhd9z26h-1mhl4b3x04l2vutj87olww5cn <node1ip>:2377
        + node1% sudo docker node ls; # lists 3 nodes where node1 is manager and node 2 and node3 are workers
        + node2% sudo docker node ls; # errors out; node2 and node3 are workers so can't run swarm commands
        + node1% sudo docker node update --role manager node2; # upgrading node2 to manager
        + node2% sudo docker node ls
        + node1% sudo docker swarm join-token manager; # get manager token
        + node3% docker swarm join --token SWMTKN-1-2th56plepacrp8d8giiyqrtv86m803ia7kiecrzog9xhd9z26h-e9fhwpe6yhxb8boa2tix8nv6l <node1ip>:2377; # adding node3 directly as manager
        + node1% sudo docker node ls
        + node1% sudo docker service create --replicas 3 alpine ping 8.8.8.8
        + node1% sudo docker service ls; # list of services
        + node1% sudo docker node ps; # list container runs on node1
        + node1% sudo docker node ps node2; # list container runs on node2
        + node1% sudo docker service ps <service name>; # list of container runs, this is due to routing mesh
        + node1% sudo docker service inspect drupal; # list only node1 ip overlay
        + node1% sudo docker node ls
        + node1% sudo docker node demote node3
        + node3% sudo docker swarm leave
        + node3% sudo docker node rm node3
        + node1% sudo docker node ls
        + node1% sudo docker node demote node2
        + node2% sudo docker swarm leave
        + node2% sudo docker node rm node2
        + node1% sudo docker swarm leave --force; # use --force only on last manager node to leave
      + resources
        + https://www.digitalocean.com/?refcode=b813dfcad8d4&utm_campaign=Referral_Invite&utm_medium=Referral_Program&utm_source=CopyPaste - digital ocean coupon for $10
        + https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets - create and upload a SSH key to digital ocean
        + https://www.bretfisher.com/docker-swarm-firewall-ports/ - docker swarm firewall ports
        + https://www.digitalocean.com/community/tutorials/how-to-configure-custom-connection-options-for-your-ssh-client - configure SSH for saving options for specific connections
***** scaling out with overlay networking
      + '--driver overlay' - creates swarm wide bridge network
      + only for container-to-container traffic inside a single swarm
      + optional IPSec (AES) encryption on network creation
      + each service can be connected to multiple networks (e.g. front-end, back-end)
      + node1% sudo docker network create --driver overlay mydrupal
      + node1% sudo docker network ls
      + node1% sudo docker service create --name psql --network mydrupal -e POSTGRES_PASSWORD=mypasswd postgres; # service can be launched on either of node1, node2 or node3
      + node1% sudo docker service ls
      + node1% sudo docker service ps psql; # psql running on node2
      + node2% sudo docker container logs <container name>
      + node1% sudo docker service create --name drupal --network mydrupal -p 80:80 drupal
      + node1% sudo docker service ls
      + node1% sudo docker service ps drupal; # drupal running on node1
      + node1% watch sudo docker service ls; # linux command watch, to watch services
      + in browser https://<node1ip>; # drupal install; overlay acts like every thing on same sub net
      + all 3 (node1ip, node2ip and node3ip) ip's display drupal website
***** scaling out with routing mesh
      + routing mesh
        + routes ingress (incoming) packets for a service to proper task
        + spans all nodes in swarm
        + uses IPVS from linux kernel
        + load balances swarm services across thier tasks
        + two way this works:
          + container-to-container in a overlay network (uses VIP)
          + external traffic incoming to published ports (all nodes listen)
        + this is stateless load balancing
        + this LB is at OSI layer 3 (TCP), not Layer 4 (DNS)
        + both limitation can be overcome with
          + Nginx or HAProxy LB proxy
          + Docker EE, which comes with built-in L4 web proxy
      + node1% sudo docker service create --name search --replicas 3 -p 9200:9200 elasticsearch:2
      + node1% sudo docker service ps search; # list search ps
      + node1% curl localhost:9200; # elasticsearch basic info
      + node1% curl localhost:9200; # now on different node showing load balancing
      + resources
        + https://docs.docker.com/engine/swarm/ingress/ - use swarm mode routing mesh (docker docs)
***** assignment answers: create a multi-service multi-node web app
      + docker's distributed voting app
      + % cp material/udemy-docker-mastery/swarm-app-1 work/. && cd work/swarm-app-1
      + 1 volume, 2 networks and 5 services needed
      + create the commands needed, spin up services and test app
      + everything is using docker hub images, so no data needed on swarm; don't build on production swarms/cloud as they eat resources)
      + like many computer things, this is 1/2 art and 1/2 science
      + node1% sudo docker service ls; # no services
      + node1% sudo node ls; # node1, node2 and node3 available as managers
      + edit README.md and following instructions
      + node1% sudo docker network create -d overlay backend; # create backend network
      + node1% sudo docker network create -d overlay frontend; # create frontend network
      + node1% sudo docker service create --name vote -p 80:80 --network frontend --replicas 2 dockersamples/examplevotingapp_vote:before
      + node1% sudo docker service create --name redis --network frontend --replicas 1 redis:3.2
      + node1% sudo docker service create --name worker --network frontend --network backend dockersamples/examplevotingapp_worker
      + node1% sudo docker service create --name db --network backend --mount type=volume,source=db-data,target=/var/lib/postgresql/data postgres:9.4
        + '-v' is not support in docker service, it's replaced with '--mount'
      + node1% sudo docker service create --name result --network backend -p 5001:80 dockersamples/examplevotingapp_result:before
      + node1% sudo docker service ls; # all five services are running
      + node1% sudo docker service ps db; # runs on node3
      + node1% sudo docker service ps redis; # runs on node3
      + node1% sudo docker service ps result; # runs on node2
      + node1% sudo docker service ps worker; # runs on node2 and node1
      + node1% sudo docker service logs worker; # runs on node1; worker fails until db is up but service is re-started
      + node1% sudo docker container logs <workercontainername>
      + In browser <node1ip> to vote and <node2ip>:5001 to check result
***** swarm stacks and production grade compose
      + stacks: production grade compose
        + stacks accpt compose files as their declarative definition for services, networks and volumes
        + 'docker stack deploy' rather then docker service create
        + stacks manages all those objects for us, including overlay network per stack. Adds stack name to start of their name
        + new 'deploy:' key in compose file. can't do build: - building shouldn't happen on swarm, it should happen by CI tool jenkins and upload to repository
        + docker-compose now ignores 'deploy:' (on local machine), docker swarm ignores 'build:'
        + docker-compose cli not needed on swarm server - it's not a production tool, it's used on local m/c
        + stack is only for one swarm
      + node1% sudo docker service ls; # lists services
      + node1% sudo docker service rm db redis result vote worker; # remove services
      + % cp material/udemy-docker-master/swarm-stack-1 work/. && cd work/swarm-stack-1
      + % scp -r example-voting-app-stack.yml root@node1ip:.
      + node1% sudo docker stack deploy -c example-voting-app-stack.yml voteapp; # deploy's app
      + node1% sudo docker stack --help; # docker stack help
      + node1% sudo docker stack ls; # lists stack's and it's services
      + node1% sudo docker stack ps voteapp; # lists processes
      + node1% sudo docker stack services voteapp; # lists services; this and above command are useful
      + node1% sudo docker network ls; # lists network
      + in browser <node3ip>:5000 for vote and <node2ip>:5001 for results; # vote is on node3 and result on node2
      + in browser <node3ip>:8080 for visualize; # visualizer is on node3; gives details of nodes and it's service details in visual form
      + change vote replicas to 3 in example-voting-app-stack.yml for update
      + node1% sudo docker stack deploy -c example-voting-app-stack.yml voteapp; # updates services
      + resources
        + https://docs.docker.com/compose/compose-file/#not-supported-for-docker-stack-deploy - features not supported in stack deploy
***** secrets storage for swarm: protecting your environment variables
      + secrets storage
        + easiest "secure" solution for storing secrets in swarm
        + secrets are - usernames and passwords; TLS certificates and keys; SSH keys ...
        + supports generic strings or binary content up to 500kb in size
        + doesn't require app to be re-written
        + as of docker 1.13.0 swarm raft DB is encrypted on disk
        + only stored on disk on manager nodes
        + default is managers and workers "control plane" is TLS + Mutual Auth
        + secrets are first stored in swarm db, then assigned to a service(s)
        + only containers is assigned service(s) can see them
        + they look like files in container but are actually in-memory fs
        + /run/secrets/<secret_name> or /run/secrets/<secret_alias>
        + local docker-compose can use file-based secrets, but not secure - fakes secrets, secrets are used to run on swarm
***** using secrets in swarm services
      + % cp material/udemy-docker-mastery/secrets-sample-1 work/. && cd work
      + % scp -r secrets-sample-1 root@node1ip:.
      + node1% cd secrets-sample-1
      + node1% sudo docker secret create psql_user psql_user.txt
      + node1% echo "myDBpassWORD" | sudo docker secret create psql_pass - ; # '-' is read from input; not safe as it's in history
      + node1% sudo docker secret ls; # lists secret
      + node1% sudo docker secret inspect psql_user; # info about psql_user
      + node1% sudo docker service create --name psql --secret psql_user --secret psql_pass -e POSTGRES_PASSWORD_FILE=/run/secrets/psql_pass -e POSTGRES_USER_FILE=/run/secrets/psql_user postgres; # run service using secrets
      + node1% sudo docker service ps psql
      + node1% sudo docker exec -it <containername> bash
      + c% cat /run/secrets/psql_user; # on node3 as psql launched on node3
      + c% cat /run/secrets/psql_pass
      + c% exit
      + resources
        - https://docs.docker.com/engine/swarm/secrets/ - manage sensitive data with docker secrets (docker docs) (lots of good reading and example)
***** using secrets with swarm stacks
      + % cp material/udemy-docker-mastery/secrets-sample-2 work/. && cd work
      + % scp -r secrets-sample-2 root@node1ip:.
      + node1% cd secrets-sample-2
      + node1% sudo docker stack deploy -c docker-compose.yml mydb; # run secrets via stack (compose)
      + node1% sudo docker stack rm mydb; # removes secrets as well
      + resources
        + https://docs.docker.com/compose/compose-file/#secrets-configuration-reference - secrets in compose files (docker docs)
***** using secrets with local docker compose
      + % cd work/secrets-sample-2; # on local machine
      + % sudo docker-compose up -d; # run docker-compose
      + % sudo docker-compose exec psql cat /run/secrets/psql_user.txt; # run exec command on psql through docker-compose; compose not secure but works on local
      + % sudo docker-compose exec psql cat /run/secrets/psql_password.txt; # run exec command on psql through docker-compose; usefully to test locally
***** assignment answers: create a stack with secrets and deploy
      + % cp compose-assignment-2 secrets-assignment-1
      + stack and compose works on docker-compose.yml version '3.1'
      + edit secrets-assignmet-1/docker-compose.yml
      + % scp -r secrets-assignment-2 root@node1ip:.
      + node1% echo "abcxyz" | docker secret create psql-pw -
      + node1% sudo docker stack deploy -c docker-compose.yml drupal
      + node1% sudo docker stack ps 
      + in browser http://<node1ip>:8080
***** full app lifecycle: dev, build and deploy with a single compose design
      + full app lifecycle with compose
        + swarm+stacks+secrets are good features for production
        + local 'docker-compose up' development environment
        + remote 'docker-compose up' CI environment
        + remote 'docker stack deploy' production environment
      + % cp -rf materials/udemy-docker-mastery/swarm-sample-3 work %% cd work/swarm-sample-3
      + 'Dockerfile' - build image
      + 'docker-compose.yml' - called in all docker-compose.*.yml files
      + 'docker-compose.override.yml' - local development, docker-compose automatically reads this and overrides docker-compose.yml; has bind-mount of themes
        + file secret
      + 'docker-compose.prod.yml' - production environment, need to specify file manually; no bind-mount; need to use docker config
        + external secret
      + 'docker-compose.test.yml' - test environment, need to specify file manually; docker-compose -f
        + jenkin CI solution run on commit; no need to define volumes because no need to keep named volumes because as soon as the test passes or fails the data is removed
        + sample_data scenario, in CI solution may be comes from custom data repository. instead of creating database every single time for testing we bind mount sample data 
        + file secret
      + % sudo docker-compose up -d; # deveopment environment: uses docker-compose.yml first then overlay docker-compose.override.yml
      + % sudo docker inspect drupal; # all the mounts installed
      + % sudo docker-compose down
      + % sudo docker-compose -f docker-compose.yml -f docker-compose.test.yml up -d; # test environment; no bind mounts as we didn't specify
      + % sudo docker-compose down
      + % sudo docker-compose -f docker-compose.yml -f docker-compose.prod.yml config > output.yml; # prod environment; output.yml is used in production stack
      + % scp -r output.yml root@<node1ip>:.
      + node1% sudo docker stack deploy -c output.yml drupal
      + Note: compose extends: another way to override file, doesn't work yet in stacks
      + resources
        + https://docs.docker.com/compose/extends/#multiple-compose-files - using multiple compose files (docker docs)
        + https://docs.docker.com/compose/production/ - using compose files in production (docker docs)
**** container registries: image storage and distribution
***** docker hub: digging deeper
      + container registries
        + an image registry needs to be part of your container plan
      + docker hub
        + most popular public image registry
        + docker registry+lightweight image building
        + 'hub.docker.com' # docker hub link
          + one private free
          + "webhooks" - images are build and pushed to docker hub; send webhooks to jenkins CI for auto build for automation
          + "collaborators" - permissions for other users
          + can create organisations which is free
          + if you are using github/bitbucket don't use hub.docker.com create repository
            + use 'create automated build' - allows CI path to github/bitbucket to build images based on code commits (reverse webhooks)
            + tags images with "automated build"
            + go through 'build details' and 'build settings'
            + in 'build settings' for FROM image; add Repository link to that image; so that any change in that image it builds image
            + 'build triggers' - some other software not github to trigger build
      + resources
        + https://hub.docker.com - docker hub
***** docker store: what is it for ?
      + store.docker.com
        + download docker "editions"
        + find certified docker/swarm plugins and commercial certified images
        + no different versions of images
        + 'dockerhub' is 'git hub' of images; 'docker store' is like app store for images
      + resources
        + https://store.docker.com - docker store
***** docker cloud: CI/CD and server ops
      + docker cloud
        + web based Docker Swarm creation/management
        + uses popular cloud hosters (aws, do) and bring-your-own-server and don't want to use cli and manage
        + you can pay for automated image building, testing and deployment - CI/CD platform
        + includes a image security scanning service
        + for official images in docker hub under tags you can see vunerabilities (which are inter run by docker cloud security ???)
      + resources
        + https://cloud.docker.com - docker cloud
***** use docker cloud for easy remote swarm management
      + resources
        + https://www.youtube.com/watch?v=VJmbCioYKGg&feature=youtu.be - fleet management and collaboration
***** understanding docker registry
      + docker regitry
        + image storage and distribution
        + https://github.com/docker/distribution - docker source code
        + https://hub.docker.com/_/registry/ - official docker repository
        + at its core: a web API and storage system, written in Go
        + storage supports local, S3/Azure/Alibaba/Google Cloud and OpenStack Swift
        + look in section resources for links to:
          + secure your registry with TLS
          + storage cleanup via Garbage Collection
          + enable hub caching via "--registry-mirror" - like a proxy to download image once and used by other machines
      + resources
        + https://docs.docker.com/registry/configuration/ - registry configuration docs
        + https://docs.docker.com/registry/garbage-collection/ - registry garbage collection
        + https://docs.docker.com/registry/recipes/mirror/ - use registry as a "mirror" of docker hub
***** run a private docker registry
      + using docker registry local
      + registry and proper TLS
        + "secure by default": docker won't talk to registry without HTTPS
        + except, localhost (127.0.0.0/8)
        + for remote self-signed TLS, enable "insecure-registry" in engine - not recommended
      + % cp -rf material/udemy-docker-mastery/registry-sample-1 work/. && cd work/registry-sample-1
      + % sudo docker container run -d -p 5000:5000 --name registry registry
      + % sudo docker container ls
      + % sudo docker image ls
      + % sudo docker pull hello-world
      + % sudo docker run hello-world
      + % sudo docker tag hello-world 127.0.0.1:5000/hello-world; # own official image at root local registry
      + % sudo docker image ls
      + % sudo docker push 127.0.0.1:5000/hello-world; # pushes to local registry
      + % sudo docker image remove hello-world
      + % sudo docker container rm <containername>
      + % sudo docker remove 127.0.0.1:5000/hello-world; # image removed
      + % sudo docker pull 127.0.0.1:5000/hello-world; # pull's from local registry
      + % sudo docker container kill registry
      + % sudo docker container rm registry
      + % sudo docker container run -d -p 5000:5000 --name registry -v $(pwd)/registry-data:/var/lib/registry registry; # new registry with new database
      + % sudo docker image ls
      + % sudo docker push 127.0.0.1:5000/hello-world
      + % ll registry-data
      + % tree registry-data/ # shows registry
        + "tree" show dirs and files visually
          + linux: apt/yum install tree
          + macOS: brew install tree
          + windows: already installed
        + show data, blob (binaries) and meta data
      + summarize: run a private docker registry recap
        + run the registry image
          + 'docker container run -d -p 5000:5000 --name registry registry'
        + re-tag an existing image and push it to your registry
          + 'docker tag hello-world 127.0.0.1:5000/hello-world'
          + 'docker push 127.0.0.1:5000/hello-world'
        + remove that image from local cache and pull it from new registry
          + 'docker image remove hello-world'
          + 'docker image remove 127.0.0.1:5000/hello-world'
          + 'docker pull 127.0.0.1:5000/hello-world'
        + re-create registry using a bind mount and see how it stores data
          + 'docker container run -d -p 5000:5000 --name registry -v $(pwd)/registry-data:/var/lib/registry registry'
***** assignment: secure docker registry with TLS and authentication
      + default registry installed is rather bare bones, and is open by default, meaning anyone can push and pull images
      + we should at lesat add TLS to it so you can work with it easily via HTTPS and then also add some basic authentication
      + try it on play-with-docker
        + http://training.play-with-docker.com/linux-registry-part2/ - docker registry for linux parts 2 & 3
        + http://training.play-with-docker.com/linux-registry-part1/ - docker registry for linux part 1
        + http://training.play-with-docker.com/ - for more extra credit labs
***** using docker registry with swarm
      + private docker registry with swarm
        + works the same way as localhost
        + because of routing mesh, all nodes can see 127.0.0.1:5000
        + remember to decide how to store images (volume driver)
        + go to play-with-docker.com
          + go to 'setings' and click 5 managers and no workers
          + % docker node ls; # 5 nodes are running
          + % docker service create --name registry --publish 5000:5000 registry
          + % docker service ps regsitry; # registry is running; in browser click the port 5000 and add ....com/v2/_catalog returns a simple json array (as registry is empty)
          + % docker pull hello-world
          + % docker tag hello-world 127.0.0.1:5000/hello-world
          + % docker push 127.0.0.1:5000/hello-world
          + % docker image rm hello-world 127.0.0.1:5000/hello-world; # in browser click the port 5000 and add ....com/v2/_catalog returns a json array with one entry hello-world
          + % docker pull nginx
          + % docker tag nginx 127.0.0.1:5000/nginx
          + % docker push 127.0.0.1:5000/nginx
          + % docker service create --name nginx -p 80:80 --replicas 5 --detach=false 127.0.0.1:5000/nginx; # shows command running in real time
            + because it's run as a service, all node able to push and pull images from local repository
          + % docker service ps nginx; # 1 running one each node and with local image
          + all nodes must be able to access image, routed mesh makes it easy
          + ProTip: use a hosted SaaS registry if possible - try these first and then only local hosted platform because of utility
***** third party image registry
      + image registry
        + docker hub
        + docker enterprise edition DTR (docker trusted registry)
        + docker registry
        + Quay.io - popular choice and comparable to docker hub as cloud-based registry
          + https://sysdig.com/blog/sysdig-docker-usage-report-2017/ - sysdig report on registry usage
        + aws, azure and google cloud have their own registry options well integrated with their toolset
        + self-hosted options
          + Docker EE
          + Quay Enterprise
          + GitLab which comes with GitLab container registry
            + https://about.gitlab.com/2016/05/23/gitlab-container-registry/
        + more list of registries
          + https://github.com/veggiemonk/awesome-docker#hosting-images-registries - list of docker resources
**** bonus section
     + bret's dockercon 2017 video: "Journey to Docker Production"
       + https://www.youtube.com/watch?v=ZdUcKtg84T8
       + https://dockercon.docker.com/watch/WdAeLaLuSCNQwEp61YVXUt
     + swarm quorum and recovery (laura frank from dockercon 2017)
       + https://www.youtube.com/watch?v=Qsv-q8WbIZY
     + bret's docker and devoops newsletter
       + https://www.getdrip.com/forms/54453022/submissions/new - register here
     + using prune to keep your docker system clean
       + https://www.youtube.com/watch?v=_4QzP7uwtvI&feature=youtu.be
     + what's new in docker 17.06
       + https://www.youtube.com/watch?v=-NeaXUGEK_g
       + https://github.com/docker/docker-ce/releases - detailed list of changes
     + Node.js Good defaults for docker
       + https://github.com/BretFisher/node-docker-good-defaults - github repo, go through README.md; best setup a Node app in Docker for local development and production use
** kubernetes
*** udemy - learn devops: the complete kubernetes course - edward viaene
**** course introduction
     + course overview
       + introduction
         + what is kubernetes - cloud/on-premise setup; cluster setup; building containers; running your first app; building container images
       + kubernetes basics
         + node architecture; scaling pods; deployments; services; labels; healthchecks; secrets; WebUI
       + advanced topics
         + service auto-discovery; configmap; ingress; volumes; pet sets; daemon sets; monitoring; autoscaling
       + administration
         + master services; quotas and limits; namespace; user management; networking; node maintenance; high availability
     + course repo
       + https://github.com/wardviaene/kubernetes-course
     + procedure document
       + download kubectl
         + linux - https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubectl
         + macos - https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/darwin/amd64/kubectl
         + windows - https://github.com/eirslett/kubectl-windows/releases/download/v1.6.3/kubectl.exe
       + minikube
         + project url - https://github.com/kubernetes/minikube
         + download instructions - https://github.com/kubernetes/minikube/releases
         + virtual box - http://www.virtualbox.org
         + windows
           + download latest minikube-version.exe
           + rename the file to minikube.exe and put it in C:\minikube
           + open a cmd and run 'cd C:\minikube' and enter 'minikube start'
           + test your commands
             + 'minikube status'
             + 'kubectl run hello-minikube -- image=gcr.io/google_containers/echoserver:1.4 --port=8080 kubectl expose deployment hello-minikube --type=NodePort'
             + 'minikube service hello-minikube --url'
       + kops
         + project url: https://github.com/kubernetes/kops
       + free dns service
         + http://freedns.afraid.org/
         + choose for subdomain hosting
         + enter aws nameservers give to you in route53 as nameservers for the subdomain
         + http://www.dot.tk/ - provides a free.tk domain name you can use and you can point it to the amazon aws nameservers
         + namecheap.com often has promotions for tld's like .co for just a couple of bucks
       + cluster commands
         + 'kops create cluster --name=kubernetes.newtech.academy --state=s3://kops-state-b429b --zones=eu-west-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=kubernetes.newtech.academy'
         + 'kops update cluster kubernetes.newtech.academy --yes --state=s3://kops-state-b429b'
         + 'kops delete cluster --name kubernetes.newtech.academy --state=s3://kops-state-b429b'
         + 'kops delete cluster --name kubernetes.newtech.academy --state=s3://kops-state-b429b --yes'
       + kubernetes from scratch
         + you can setup your cluster manually from scratch
         + if you're planning to deploy on AWS/Google/Azure, use the tools that are fit for these platforms
         + if you have an unsupported cloud platform, and you still want kubernets, you can install it manually
         + coreOS + kubernetes
           + https://coreos.com/kubernetes/docs/latest/getting-started.html
       + docker
         + download docker engine
           + windows:  https://docs.docker.com/engine/installation/windows/
           + macos: https://docs.docker.com/engine/installation/mac/
           + linux: https://docs.docker.com/engine/installation/linux/
       + devops box
         + virtualbox: http://www.virtualbox.org
         + vagrant: http://www.vagrantup.com
         + devops box: https://github.com/wardviaene/devops-box
           + vagrant project with an ubuntu box with the tools needed to do DevOps
           + tools included: Terraform, aws cli, ansible
         + launch commands (in terminal)
           + 'cd devops-box && vagrant up'
         + launch commands for a plain ubuntu
           + 'mkdir ubuntu'
           + 'vagrant init ubuntu/xenial64'
           + 'vagrant up'
       + docker commands
         + 'docker build' - build image
         + 'docker build -t wardviaene/k8s-demo:latest .' - build & tag
         + 'docker tag imageid wardviaene/k8s-demo' - tag image
         + 'docker push wardviaene/k8s-demo' - push image
         + 'docker images' - list images
         + 'docker ps -a' - list all containers
       + kubernetes commands
         + 'kubectl get pod' - get info about all running pods
         + 'kubectl describe pod <pod>' - describe one pod
         + 'kubectl expose pod <pod> --port=444 --name=frontend' - expose the port of a pod (creates a new service)
         + 'kubectl port-forward <pod> 8080' - port forward the exposed pod port to your local machine
         + 'kubectl attach <podname> -i' - attach to the pod
         + 'kube exec <pod> --command' - execute a command on the pod
         + 'kubectl label pods <pod> mylabel=awesome' - add a new label to a pod
         + 'kubectl run -i --tty busybox --image=busybox --restart=Never -- sh' - run a shell in a pod - very useful for debugging
         + 'kubectl get deployments' - get information on current deployments
         + 'kubectl get rs' - get information about the replica sets
         + 'kubectl get pods --show-labels' - get pods, and also show labels attached to those pods
         + 'kubectl rollout status deployment/helloworld-deployment' - get info on current deployments
         + 'kubectl get rs' - get info about the replica sets
         + 'kubectl get pods --show-labels' - get pods, and also show labels attached to those pods
         + 'kubectl rollout status deployment/helloworld-deployment' - get deployment status
         + 'kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo-2' - run k8s-demo with the image label vesion 2
         + 'kubectl edit deployment/helloworld-deployment' - edit the deployment object
         + 'kubectl rollout history deployment/helloworld-deployment' - get the rollout history
         + 'kubectl rollout undo deployment/helloworld-deployment' - rollback to previous version
         + 'kubectl rollout undo deployment/helloworld-deployment --to-version=n' - rollback to any version
       + aws commands
         + 'aws ec2 create-volume --size 10 --region us-east-1 --availability-zone us-east-1a --volume-type gp2'
       + certificates
         + 'openssl genrsa -out myuser.pem 2048' - creating a new key for a new user
         + 'openssl req -new -key myuser.pem -out myuser -csr.pem -subj "/CN=myuser/O=myteam/" - creating a certificate request
         + 'openssl x509 -req -in myuser-csr.pem -CA /path/to/kubernetes/ca.crt -CAkey /path/to/kubernetes/ca.key - CAcreateserial -out myuser.crt -days 10000' - creating a certificate
**** introduction to kubernetes
***** kubernetes introduction
      + kubernetes is an open source orchestration system for docker containers
        + schedules containers on a cluster of machines
        + can run multiple containers on one machine
        + can start the container on specific nodes
        + can move containers from one node to another node
        + clusters can start with one node until thousands of nodes
        + other docker orchestrators: docker swarm (not as extensive as kubernetes), mesos
        + can run on-premise (own datacenter), public (aws, gcp), hybrid (public & private)
        + highly moduler
        + open source and available on github
        + originally developed by google called borg and released to open source
***** containers introduction
      + vm: has a guest OS on host OS with hypervisor (phy server->hostOS->Hypervisor->GuestOS->bins/libs->apps); need full boot cycle
      + container: no guest OS; no boot cycle (phy server->hostOS->bins/libs->app with Docker Engine); no boot cycle and runs as process
        + but container on cloud provider still has Hypervisor and GuestOS but still gets all benefits of containers
        + Docker is most popular container software
          + rkt - another container software works with kubernetes as well
        + Docker Engine is Docker runtime to make run docker images
        + Isolation: you ship a binary with all the dependencies
        + You can run the same docker image, unchanged, on laptops, data center VMs and Cloud providers
        + Docker uses Linux Containers (a kernel feature) for operating system-level isolation
***** kubernetes setup
      + things like volumes and external load balancers work only with supported cloud providers
        + for aws, gcp and azure it's available
      + 'minikube' to quickly spin up a local single machine with a kubernetes cluster
      + 'kops' to spin up a highly available production cluster on aws (http://aws.amazon.com)
      + https://github.com/kubernetes/minikube - minikube github
      + can use digital ocean (https://www.digitalocean.com)
***** local setup with minikube
      + 'minikube' is a tool that makes it easy to run kubernetes locally
        + runs a single-node kubernetes cluster inside a linux vm
        + aimed on users to just test it out or use it for development
        + cannot spin up a production cluster, it's one node machine with no high availability
        + works on windows, linux and macos and need virtualization software
          + virtualbox - www.virtualbox.org
        + 'minikube start' - to start a cluster
***** demo: minikube
      + go to "https://github.com/kubernetes/minikube/releasese"
      + minikube install
        + macos:
          + % brew cask install minikube 
          + or % brew cask reinstall minikube; # to upgrade minikube
          + or % curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
        + linux:
          % curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/
        + % minikube; # lists commands
      + kubectl install
        + macos:
          + % curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl
          + % chmod +x kubectl
          + % mv kubectl /usr/local/bin/kubectl
        + linux:
          + % curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
          + % chmod +x kubectl
          + % mv kubectl /usr/local/bin/kubectl
          + % kubectl; # lists command
      + virtual box install
        + ubuntu xenial install
          + add official virtualbox repository
            + % echo "deb http://download.virtualbox.org/virtualbox/debian $(lsb_release -cs) contrib" | sudo tee /etc/apt/sources.list.d/oracle-virtualbox.list
          + download and add keys
            + % wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add -
            + % wget -q https://www.virtualbox.org/download/oracle_vbox.asc -O- | sudo apt-key add -
          + install virtualbox
            + % sudo apt-get update
            + % sudo apt-get install virtualbox-5.2
      + minikube demo
        + % minikube start --cpus 1 --memory 1024; # start minikube with cpus=1 and memory=1024MB
          + ~/.kube/config - kubectl config file created; kubectl will use this config
        + % kubectl run nginx --image=nginx --port=80; # deployment "nginx" created
          + https://cloud.google.com/container-registry/ - google container registry
          + 'kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080' - another example run
        + % kubectl expose deployment nginx --type=NodePort; # service "nginx" exposed to host
        + % kubectl get pod; # to check whether the pod is up and running, check status
          + go to browser and checker 'url'
        + % minikube service nginx --url; # as pod is running, we can now be able to use curl
          + % curl(minikube service nginx --url); # to get output in terminal
        + open url in browser
        + % minikube dashboard; # opens minikube dashboard in default browser
        + % kubectl delete service nginx; # service "nginxd" deleted
        + % kubectl delete deployment nginx; # deployment "nginx" deleted
        + % minikube stop; # stopping local kubenetes cluster
***** introduction to kops
      + kops - kubernetes operations, to setup kubernetes on aws
      + tool allows you to do production grade kubernetes installations, upgrades and management
      + there is also a leagcy tool called kube-up.sh - it's now deprecated
      + kops only works on mac/linux
      + on windows
        + need to a virtual machine first (www.virtualbox.org)
        + use vagrant to boot linux box
          + % mkdir ubuntu; # via cygwin
          + % cd ubuntu
          + % vagrant init ubuntu/xenial64
          + % vagrant up
          + % vagrant ssh
        + and use kops
***** demo: preparing aws for kops install
      + https://github.com/kubernetes/kops - kops github link
      + kops and awscli install
        + mac
          + % brew update && brew install kops
          + % easy_install pip
          + % pip install --upgrade pip
          + % pip install awscli
        + linux
          + % curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
          + % chmod +x kops-linux-amd64
          + % sudo mv kops-linux-amd64 /usr/local/bin/kops
          + % sudo apt-get install -y python-pip
          + % sudo pip install awscli
      + aws.amazon.com; # open free tier account
        + create a user in IAM with admin access; copy 'AccessKeyID' and 'SecretAccessKey'; # need for awscli usage
          + enable MFA and console access with custom passwd for the user
        + % aws configure
          + enter copied "AccessKeyID", "SecretAccessKey". Enter region as 'ap-south-1' and default output format as 'json'
          + creates files in ~/.aws/
        + create s3 bucket in region "ap-south-1" (Mumbai)
          + cloudping.info to check latency
        + setup dns
          + kops will setup dns but we need to setup sub-domain 
          + route53 allows us to do dns management
          + kops expect to enter sub-domain which it manages
          + route53 hosted zone is charged at 0.5$ per month
            + namecheap.com, freenom.com - get domains
            + create hosted zones
            + add name servers to freenom/namecheap domain
***** demo: cluster setup on aws using kops
      + % ssh-keygen; # generate ssh-key; .ssh/id_rsa.pub is public key
      + % kops create cluster --name=yar-aws.tk --state=s3://<s3bucket> --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=yar-aws.tk; # creates 1 master and 2 nodes; give name as default dns name
        + % kops edit cluster yar-aws.tk --state=s3://<s3bucket>; # if need to edit further
      + % kops update cluster yar-aws.tk --yes --state=s3://<s3bucket>; # to launch cluster
        + certificate and passwd is updated in ~/.kube/config
      + % kubectl get node; # list of nodes; 1 master and 2 nodes
      + % kubectl run hello-minikube --image=gcr.io/google_containers/echoserver:1.4 --port=8080; # run earlier example
      + % kubectl expose deployment hello-minikube --type=NodePort; # expose deployment
      + % kubectl get service; # once image is up, it assigns a port
        + if want to expose directly without load balancer we need to open this port in firewall
        + in 'VPC' management console->securitygroups->click master node->inbound rules:'add custom tcp rule', 'add port' to accept all traffic '0.0.0.0/0'
      + % kops delete cluster --name yar-aws.tk --state=s3://<s3bucket>; # display info about delete cluster
      + % kops delete cluster --name yar-aws.tk --state=s3://<s3bucket> --yes; # to confirm
***** demo: building docker containers
      + to build containers, use Docker Engine
      + download docker engine
        + windows: https://docs.docker.com/engine/installation/windows/
        + macos: https://docs.docker.com/engine/installation/mac/
        + linux: https://docs.docker.com/engine/installation/linux/
      + can use vagrant devops-box with comes with docker installed
      + % git clone https://github.com/wardviaene/docker-demo
      + % cd docker-demo
      + % sudo docker build .
      + % sudo docker container run -p 3000:3000 -it <image>
      + % curl localhost:3000
***** demo: pushing docker image
      + docker can be run locally for development purposes
      + to make an image available to kubernetes, you need to push the image to a docker registry, like docker hub
      + you can build and deploy any application you want using docker and kubernetes
      + but try to run one process in one container
      + don't try to create one gaint docker image for you app, but split it up if necessary
      + all the data in the container is not preserved, when a container stops, all the changes within a container are lost
      + you can preserve data, using volumes
      + for more tips, check out the 12-factor app methodology https://12factor.net
      + few official images you might use for your app
        + https://hub.docker.com/_/nginx/ - webserver
        + https://hub.docker.com/_/php/ - php
        + https://hub.docker.com/_/node/ - NodeJS
        + https://hub.docker.com/_/ruby/ - ruby
        + https://hub.docker.com/_/python/ - python
        + https://hub.docker.com/_/openjdk/ - java
      + % sudo docker login; # create docker login at hub.docker.com and use those credentials
      + create a new repository <user>/docker-demo
      + % sudo docker tag <imageid> <user>/docker-demo
      + % sudo docker push <user>/docker-demo
***** demo: running first app on kubernetes
      + 'pod' describes an application running on kubernetes
      + a 'pod' can contain one or more tightly coupled containers that make up the app
      + those apps can easily communicate with each other using their local port numbers
      + useful commands
        + 'kubectl get pod' - get info about running pods
        + 'kubectl describe pod <pod>' - describe one pod
        + 'kubectl expose <pod> --port=444 --name=frontend' - expose the port of a pod (creates a new service)
        + 'kubectl port-forward <pod> 8080' - port forward the exposed pod port to your local machine
        + 'kubectl attach <podname> -i' - attach to the pod
        + 'kubectl exec <pod> -- command' - execute a command on the pod
        + 'kubectl label pods <pod> mylabel=awesome' - add a new label to a pod
        + 'kubectl run -i -tty busybox --image=busybox --restart=Never -- sh' - run a shell in a pod - very useful for debugging
      + % minikube start --cpus 1 --memory 1024; # start minikube cluster
      + % cp -rf material/material/kubernetes-course/first-app work/ && cd work
      + % kubectl get node; # minikube running
      + % kubectl create -f first-app/helloworld.yml; # pod created
      + % kubectl get pod; # pod running
      + % kubectl describe pod nodehelloworld.example.com; # pod details, events and status
      + % kubectl port-forward nodehelloworld.example.com 8081:3000; # port forward host 8081 to pod container 3000
      + % curl localhost:8081; # to quickly test
        + another way to test is to create service
        + if running minikube, you can expose the port and connect directly, on aws we use loadbalancer which is always better
      + % kubectl expose pod nodehelloworld.example.com --type=NodePort --name nodehelloworld-service; # we can access port 3000 directly; service expose
        + we need to know ip address of kubernetes we need to connect to, on aws we just use master node but we need to security group
      + % minikube service nodehelloworld-service --url; # to know ip in minikube
      + % kubectl get service; # we see new service is created and running that expose port
***** demo: useful commands
      + % kubectl attach nodehelloworld.example.com; # attach to the pod
      + % kubectl exec nodehelloworld.example.com -- ls /app; # execute a command in the pod
      + % kubectl exec nodehelloworld.example.com -- touch /app/test.txt; # creates test.txt, but will be lost if container is killed
      + % kubectl exec nodehelloworld.example.com -- ls /app; # execute a command in the pod
      + % kubectl get service; # get service
      + % kubectl describe serivce nodehelloworld-service; # describe service, displays Endpoints
      + % kubectl run -i --tty busybox --image=busybox --restart=N; # create a new pod and container, gives a shell
      + p% telnet 172.17.0.5 3000; # telnet to Endpoint
      + p% GET /; # we get response, this is another way of connecting to another pod within same cluster; we can use service discovery (dns) and avoid this manual end points
***** demo: service with aws elb loadbalancer
      + in real world, you need to be able to access the app from outside
      + on aws, you can easily add an external load balancer; the aws load balancer will route the traffic to the correct pod in kubernetes
      + other solution (without load balancer) is your own haproxy/nginx load balancer in front of your cluster
      + or expose ports directly
      + in route53, create hosted zone; 
      + % kops create cluster --name=yar-aws.tk --state=s3://<s3bucket> --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=yar-aws.tk; # create aws cluster
      + % kops update cluster yar-aws.tk --state=s3://<s3bucket> --yes
      + % kubectl get node; # get nodes, wait until nodes are ready
      + % kubectl create -f first-app/helloworld.yml; # helloworld app to create pod and container
      + % kubectl create -f first-app/helloworld-service.yml; # loadbalancer service yml file
      + % kubectl get pod; # wait for the pod to running
      + % kubectl get service; # lists service with load balancer
      + in aws console, you see one load balancer; then create record set for the hosted zone and alias to load balancer in aws console
      + % kops get cluster --state=s3://<s3bucket>; # get clusters
      + % kops delete cluster --name yar-aws.tk --state=s3://<s3bucket>; # display info about delete cluster
      + % kops delete cluster --name yar-aws.tk --state=s3://<s3bucket> --yes; # to confirm
        + delete record set as load balancer is not available
**** kubernetes basics
***** demo: replication controller
      + if your application is stateless you can horizontally scale it; stateless, it doesn't write any local files/keeps local sessions
        + all traditional databases (mysql, postgres) are stateful, they have database files that can't be split over multiple instances
      + most web applications can be made stateless; session management needs to be done outside the container (you can use memcache, redis or database to store your sessions)
        + any files that need to be saved can't be saved locally on the container; can be saved on shared storage (s3)
      + for best practices, look at https://12factor.net
        + look for courses
          + learn devops: continously delivering better software
          + learn devops: scaling apps on-premise and in the cloud
      + use volumes to still run stateful apps; stateful apps can't horizontally scale, but can run them in a single container and vertically scale (allocate more cpu/memory/disk)
      + scaling in kubernetes can be done using the replication controller
        + the replication controller will ensure a specified number of pod replicas will run at all time
        + pods created with the replica controller will automatically be replaced if they fail, get deleted or are terminated
        + using replication controller is also recommended if you just want to make sure 1 pod is always running, even after reboots
      + % minikube start --cpus 1 --memory 1024
      + % kubectl get node; # minikube is ready
      + % cp -rf /material/kubernetes-course/replication-controller work/ && cd work
      + % kubectl create -f replication-controller/helloworld-repl-controller.yml; # replicationcontroller created
      + % kubectl get pods; # 2 pods created
      + % kubectl describe pod <podname>; # pod info
      + % kubectl delete pod <podname1>; # delete pod podname1
      + % kubectl get pods; # deleted pod restarts
      + % kubectl scale --replicas=4 -f replication-controller/helloworld-repl-controller.yml; # replicationcontroller scaled
      + % kubectl get pods; # 4 pods created
      + % kubectl get rc; # lists replication controller (rc=shortform for replication controller)
      + % kubectl scale --replicas=1 rc/helloworld-controller # replicationcontroller scaled to 1
      + % kubectl get rc; # show replicationcontroller scaled back
      + % kubectl get pods; # 3 are terminated
      + % kubectl delete rc/helloworld-controller; # replicationcontroller deleted
      + % kubectl get pods; # pods terminated
        + these kubectl scaling operations are saved in backend in etcd
***** demo: deployments
      + replication set is the next-generation replication controller
        + supports new selector that can do selection based on filtering according a set of values, eg:= "environment" either "dev" or "qa"
        + not only based on equality like the replication controller, eg:= "environment" == "dev"
        + this replica set, rather than the replication controller is used by the deployment object
      + deployment declaration in kubernetes allows you to do app deployments and updates
        + when using the deployment object, you define the state of your application
        + kubernetes will make sure the clusters matches your desired state
        + using replication controller or replication set might be cumbersome to deploy apps
        + deployment object is easier to use and gives you more possibilities
        + with deployment object
          + 'create' - create a deployment (e.g. deploying an app)
          + 'update' - update a deployment (e.g. deployting a new version)
          + do 'rolling updates' (zero downtime deployments)
          + 'roll back' to a previous version
          + 'pause' / 'resume' a deployment (e.g. to roll-out to only a certain percentage)
      + useful commands
        + 'kubectl get deployments' - get info on current deployments
        + 'kubectl get rs' - get info about the replica sets
        + 'kubectl get pods --show-labels' - get pods, and also show labels attached to those pods
        + 'kubectl rollout status deployment/helloworld-deployment' - get deployment status
        + 'kubectl set image deployment/helloworld-deployment k8s-demo=k9s-demo:2'  - run k8s-demo with the image label version 2
        + 'kubectl edit deployment/helloworld-deployment' - edit the deployment object
        + 'kubectl rollout status deployment/helloworld-deployment' - get the status to the rollout
        + 'kubectl rollout history deployment/helloworld-deployment' - get the rollout history
        + 'kubectl rollout undo deployment/helloworld-deployment' - rollback to previous version
        + 'kubectl rollout undo deployment/helloworld-deployment --to-revision=n' - rollback to any version
      + % minikube start --cpus 1 --memory 1024
      + % cp -rf /material/kubernetes-course/deployment work/ && cd work
      + % kubectl create -f deployment/helloworld.yml; # deployment created
      + % kubectl get deployments; # list deployments
      + % kubectl get rs; # get resplica sets
      + % kubectl get pods; # list pods
      + % kubectl get pods --show-labels; # list pods with labels
      + % kubectl rollout status deployment/helloworld-deployment; # rollout status
      + % kubectl expose deployment helloworld-deployment --type=NodePort; # expose deployment, creates service
      + % kubectl get service; # list services
      + % kubectl describe service helloworld-deployment; # descrive service
      + % minikube service helloworld-deployment --url; # gives url
      + % curl <ip>:port; # test url
      + % kubectl set image deployment/helloworld-deployment k8s-demo=wardviaene/k8s-demo:2; # deployment image update
      + % kubectl rollout status deployment/helloworld-deployment; # deployment successfully rolled out
      + % curl <ip>:port; # test url
      + % kubectl get pods; # 3 terminated and 3 started with new image
      + % kubectl rollout history deployment/helloworld-deployment; # deployments history
      + % kubectl rollout undo deployment/helloworld-deployment; # deployment rolled back
      + % kubectl rollout status deployment/helloworld-deployment; # deployment rolled out
      + % kubectl get pods; # 3 terminated and 3 started with old image
      + % kubectl rollout history deployment/helloworld-deployment; # default show 2 history
      + % kubectl edit deployment/helloworld-deployment; # edit deployment object
        + add in "spec:" section 'revisionHistoryLimit: 100'
      + % kubectl set image deployment/helloworld-deployment k8s-demo=wardviaene/k8s-demo:2; # image updated
      + % kubectl rollout history deployment/helloworld-deployment; # shows more history items
      + % kubectl rollout undo deployment/helloworld-deployment --to-revision=3; # rolls back to revision '3' (see rollback history)
        + deployment is wayforward compared to replica sets and replicas
      + delete kubectl deployment,service and stop minikube
***** demo: services
      + service:
        + pods are very dynamic, they come and go on the kubernetes cluster
        + when using a replication controller, pods are terminated and created during scaling operations
        + when using deployments, when updating the image version, pods are terminated and new pods take the place of older pods
        + that's why pods should never be accessed directly, but always through a service
        + a 'service' is the logical bridge between the 'mortal' pods and other services or end-users
        + when using the 'kubectl expose' command earlier, you created a new service for your pod, so it could be accessed externally
        + creating a service will create an endpoint for your pods
          + a 'clusterIP': a virtual IP address only reachable from within the cluster (this is default)
          + a 'NodePort': a port that is the same on each node that is also reachable externally
          + a 'LoadBalancer': a LoadBalancer created by the cloud provider that will route external traffic to every node on the NodePort (ELB on AWS)
          + the options just shown only allow you to create virtual IPs or ports
        + there is also a possibility to use DNS names
          + 'ExternalName' can provide a DNS name for the service
            + e.g. for service discovery using DNS
            + this only works when the DNS add-on is enabled in service discovery
        + by default service can only run between ports 30000-32767, but you could change this behavior by adding the --service-node-port-range= argument to the kube-apiserver (in the init scripts)
      + % minikube start --cpus 1 --memory 1024
      + % kubectl create -f first-app/helloworld.yml; # create app
      + % kubectl get pods
      + % kubectl describe pod nodehelloworld.example.com
      + % kubectl create -f first-app/helloworld=nodeport-service.yml; # create service
      + % minkube service helloworld-service --url
        + on aws, we need to go to master node and change security groups
      + % curl <ip>:31001
      + % kubectl describe svc helloworld-service; # 'svc' short form of service
      + % kubectl get svc; # list of services
        +  shown cluster-ip (or virtual ip) can be accessed within cluster and nodeport from external
      + % kubectl delete svc helloworld-service; # delete service
      + % kubectl create -f first-app/helloworld-nodeport-service.yml
      + % kubectl describe svc helloworld-service; # cluster-ip (or virtual ip) changed but nodeport is static port
      + stop kubectl deployment, services and minikube
***** demo: nodeselector using labels
      + labels
        + labels are key/value pairs that can be attached to objects like tags in AWS used to tag resources
        + can label your objects, for instance your pod following an organizational structure 
          + key: environment - value: dev/staging/qa/prod
          + key: department - value: engineering/finance/marketing
        + labels are not unique and multiple labels can be added to one object
        + once labels are attached to an object, you can use filters to narrow down results (using 'Label Selectors')
        + using 'label selectors', you can use matching expressions to match labels
          + for instance, a particulare pod can only run on a node labeled with "environment" equals "development"
          + more complex matching: "environment" in "development" or "qa"
        + you can use labels to tag nodes, you can use label selectors to let pods only run on specific nodes
        + 2 steps required to run a pod on a specific set of nodes
          + first you tag the node; then you add a nodeSelector to your pod configuration
          + first step, add a label or multiple labels to your nodes
            + 'kubectl label nodes node1 harware=high-spec'
            + 'kubectl label nodes node2 hardware=low-spec'
          + secondly, add a pod that uses those labels
      + % minikube start --cpus 1 --memory 1024
      + % kubectl get nodes; # minikube running
      + % kubectl get nodes --show-lables; # see labels
      + % kubectl create -f deployment/helloworld-nodeselector.yml
      + % kubectl get pods
      + % kubectl get pod <podname>; # shows failed to fit in any node, not matching NodeSelector
      + % kubectl label nodes minikube hardware=high-spec; # node "minikube" labeled
      + % kubectl get nodes --show-labels
      + % kubectl get pods; # pods are running
      + % kubectl describe pod <podname>; # pod runs now
        + very useful to seggregate workload
***** demo: healthchecks
      + healthchecks
        + if application malfunctions, the pod and container can still be running, but the application might not work anymore
        + to detect and resolve problem with your application, you can run health checks
        + you can run 2 different type of health checks
          + running a command in the container periodically
          + periodic checks on a URL (HTTP) - used most of the times
        + typical production application behind a load balancer should always have health checks implemented in some way to ensure availability and resiliency of the app
        + add health check under 'livenessProbe' in yml file
      + % minikube start --cpus 1 --memory 1024
      + % kubectl create -f deployment/helloworld-healthcheck.yml; # deployment with healthchecks
      + % kubectl get pods; # get status
      + % kubectl describe pod <podname>; # show under Liveness that healthcheck are running and status
      + % kubectl get pods; # pods are killed and relaunched by kubernetes
      + % kubectl edit deployment/helloworld-deployment; # to edit deployment to change settings in yml file
***** demo: credentials (secrets) using volumes
      + secrets
        + secrets provides a way in kubernetes to distribute credentials, keys, passwords or "secret" data to the pods
        + kubernetes itself uses this secrets mechanism to provide the credentials to access the internal API, you can also use the same mechanism for your application
        + secrets is one way to provide secrets, native to kubernets
        + there are still other ways your conatainer can get its secrets if you don't want to use secrets (e.g. using an external vault services in your app)
        + can be used in following ways
          + use secrets as environment variables
          + use secrets as a file in a pod
            + this setup uses volumes to be mounted in a container, in this volume you have files
            + can be used for instance for dotenv files or your app can just read this file
        + use an external image to pull secrets (from a private image registry)
          + like pull another image and you app reads from that image
        + to generate secrets using files
          + 'echo -n "root" > ./username.txt'
          + 'echo -n "password" > ./password.txt'
          + 'kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt'
        + a secret can also be an SSH key or an SSL certificate
          + 'kubectl create secret generic ssl-certificate --from-file=ssh-privatekey=~/.ssh/id_rsa --ssl-cert-=ssl-cert=mysslcert.crt'
        + to generate secrets using yaml definitions, look at 'secrets-db-secret.yml' example (check for kind:Secret)
          + use 'echo -n "root" | base64' and 'echo -n "password" | base64' and feed values to 'secrets-db-secret.yml'
          + after creating yml file, you can use kubectl 
            + 'kubectl create -f secrets-db-secret.yml'
        + you can create a pod that exposes the secrets as environment variables
        + alternatively, you can provide the secrets in a file via volume mount
          + secrets will be stores in <volumemountpath>/db-secrets/username; <volumemountpath>/db-secrets/password
      + % minikube start --cpus 1 --memory 1024
      + % kubectl create -f deployment/helloworld-secrets.yml
      + % kubectl create -f deployment/helloworld-secrets-volumes.yml
      + % kubectl get pods
      + % kubectl describe pod <podname>; # kubernetes specific volume mounts are visible; app secret volume mounts are also populated 
      + % kubectl exec <podname> -i -t -- /bin/bash; # gives bash shell
      + p% cat /etc/creds/username; # username and password are available
      + p% cat /etc/creds/password
      + p% mount; # see all the mount points
      + p% exit
***** demo: running wordpress on kubernetes
      + % cp -rf material/kubernetes-course/wordpress work && cd work
      + https://hub.docker.com/_/wordpress/ - wordpress repository, check info; it's rich content management system
      + % minikube start --cpus 1 --memory 1024
      + % kubectl create -f wordpress/wordpress-secrets.yml; # secrets created
      + % kubectl create -f wordpress/wordpress-single-deployment-no-volumes.yml; wordpress depolyment created
      + % kubectl get pods; # get pods
      + % kubectl describe pod <podname>; # pod description
      + % kubectl create -f wordpress/wordpress-service.yml; # create service to expose ports
      + % minikube service wordpress-service --url
      + go to browser,enter url and do setup
        + it's not totally functional as the app is still stateless and data will be lost as there are no data mounts
      + % kubectl delete pod/<podname>; # pod is deleted
      + % kubectl get pods; # pod is re-launched and data is lost
***** demo: webui
      + webui
        + kubernetes comes with a Web UI you can use instead of kubectl commands
        + gives an overview of running applications on your cluster
        + you can create and modify invidual kubernetes resources and workloads (kubectl create/delete)
        + retrieve info on the state of resources (kubectl describe pod)
        + https://<kubernetes=master>/ui - to access kubernetes Web UI
          + if cannot acces it (not enabled on your deploy type) you can install it manually
            + 'kubectl create -f https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml'
            + if a passwor is asked, retrieve password 'kubectl config view'
          + if minikube
            + 'minikube dashboard'
            + 'minikube dashboard --url' - to get dashboard url
      + % minikube start --cpus 1 --memory 1024
      + % minikube dashboard --url; # shows dashboard
        + open the url in browser and explore the app
      + % kubectl create -f first-app/helloworld.yml
**** advanced topics
***** demo: service discovery
      + service discovery using DNS
        + from kubernetes 1.3, DNS is built-in service launched automatically using addon manager
        + addons are in /etc/kubernetes/addons on master node
        + DNS service can be used within pods to find other services running on the same cluster
        + multiple containers within 1 pod don't need this service, as they can contact each other directly using localhost:port
        + to make DNS work, a pod will need a service definition
        + p1% 'host app1-service' - same pod app1-service ip
        + p1% 'host app2-service' - app2-service ip of another pod p2
        + p1% 'host app2-service.default' - default stands for the default namespace pods and services can be launched in different namespaces (to logically separate your cluster)
        + p1% 'host app2-serivce.default.svc.cluster.local' - lookup on full local hostname
        + 'kubectl run -i --tty busybox --image=busybox --restart=Never -- sh'
        + 'cat /etc/resolv.conf' - gives nameserver (dns server) and options ndots
        + service discovery is important when we run cluster with lots of services
      + % minikube start --cpus 1 --memory 1024
      + % cp -rf material/kubernetes-course/service-discovery work/. && cd work
      + % kubectl create -f service-discovery/secrets.yml
      + % kubectl create -f service-discovery/database.yml
      + % kubectl create -f service-discovery/database-service.yml
      + % kubectl create -f service-discovery/helloworld-db.yml
      + % kubectl create -f service-discovery/helloworld-db-service.yml
      + % minikube service helloworld-db-service --url
      + % kubectl get pod; # 1 database and 3 pod for deployment
      + % kubectl logs <pod1name>; # check whether connected to database
      + % curl <ip>:30518; # visitor no. 1
      + % curl <ip>:30518; # visitor no. 2
      + % kubectl exec database -i -t -- mysql -u root -p ; # enter passwd
        + p%mysql% show databases;
        + p%mysql% use helloworld;
        + p%mysql% show tables;
        + p%mysql% select * from visits; # see records created by app, app is able to discover the service and connected to database
        + p%mysql% \q
      + % kubectl get svc
      + % kubectl run -i --tty busybox --image=busybox --restart=N -- sh
        + p% nslookup helloworld-db-service; # shows nslookup
        + p% nslookup database-service
        + p% nslookup helloworld-db-service
        + p% telnet helloworld-db-service 3000
          +p%t% GET /; # visitor no. 5
      + % kubectl delete pod busybox 
***** demo: configmap
      + configmap
        + configuration parameters that are not secret, can be put in a configmap in key-value pairs
        + can be read by app using: environment variables, container commandline arguments in pod configuration, using volumes
        + can also contain full configuration files: e.g an webserver config file, this file can be mounted using volumes where the application expects its config file
          + this way you can "inject" configuration settings into containers without changing the container itself
        + to generate configmap using files
          + 'cat <<EOF > app.properties
            drive=jdbc
            database=postgres
            lookandfeel=1
            otherparams=xyz
            param.with.hierarchy=xyz
            EOF'
          + 'kubectl create configmap app-config --from-file=app.properties'
        + using configmap
          + you can create a pod that exposes the ConfigMap using a volume
          + you can create a pod that exposes the ConfigMap as environment variables
        + the config values will be stored in files: /etc/config/drive; /etc/config/param/with/hierarchy
      + % minikube start --cpus 1 --memory 1024
      + % cp -rf material/kubernetes-course/configmap work/. && cd work
      + % kubectl create configmap nginx-config --from-file=configmap/reserveproxy.conf; # configmap created
      + % kubectl get configmap; # lists configmap
      + % kubectl get configmap nginx-config -o yaml; # nginx-config configmap info in yaml output format
      + % kubectl create -f configmap/nginx.yml; # uses configmap via volume mounts
      + % kubectl create -f configmap/nginx-service.yml; # run service to expose port
      + % minikube service hellowrold-nginx-service --url
      + % kubectl get pod
      + % curl https://<ip>:31264 -vvvv ; # with verbose, nginx forward to helloworld via configmap. This shows how to use nginx as a proxy
      + % kubectl exec -i -t helloworld-nginx -c nginx -- bash
        + p% ps x
        + p% cat /etc/nginx/conf.d/reverseproxy.conf
***** demo: ingress controller
      + ingress controller
        + ingress is a solution that allows inbound connections to the cluster
        + it's an alternative to the external Loadbalancer and nodePorts
        + ingress allows you to easily expose services that need to be accessible from outside to the cluster
        + with ingress you can run your own ingress controller (basically a loadbalancer) within the kubernetes cluster
        + there are default ingress controllers available, or you can write your own ingress controller
        + port 80: http; port 443: https
        + ingress rules
          + using ingress object
        + viable alternative if you don't want to run external load balancer from cloud providers as it cost money and you can add your controllers
      + % minikube start --cpus 1 --memory 1024
      + % cp -rf material/kubernetes-course/ingress work/. && cd work/
      + % kubectl create -f ingress/ingress.yml; # ingress created
      + % kubectl create -f ingress/nginx-ingress-controller.yml; # replicationcontroller created
      + % kubectl create -f ingress/echoservice.yml; # replicationcontroller created
      + % kubectl create -f ingress/helloworld-v1.yml; # deployment, service created
      + % kubectl create -f ingress/helloworld-v2.yml; # deplloyment, service created
      + % kubectl get pod
      + % minikube ip; # gives ip of the cluster
      + % curl <clusterip>; # hits default service
      + % curl <clusterip> -H 'Host: helloworld-v1.example.com'; # ingress controller forwards to helloworld
      + % curl <clusterip> -H 'Host: helloworld-v2.example.com'; # ingress controller forwards to helloworld v2
      + % kubectl get svc
***** demo: volumes
      + volumes
        + running apps with state
        + volumes in kubernetes allow you to store data outside the container
        + when container stops, all data on the container itself is lost
        + apps don't keep a local state, but store their state in an external service like database, caching server (e.g. mysql, aws s3). No need of volumes and run as service
        + persistent volumes in kubernetes allow you attach a volume to a container that will exists even when the container stops
        + volumes can be attached using different volume plugins: aws cloud (ebs storage), google cloud (google disk), network storage (nfs, cephfs), microsoft cloud (azure disk)
        + using volumes, you could deploy applications with state on your cluster, that need to read/write to files on the local filesystem that need to be persistent in time
        + you could run a mysql database using persistent volumes, although this might not be ready for production (yet)
        + volumes are new since june 2016 in kubernetes, will become stable at some point of time
        + to use volumes, you need to create the volume first
          + 'aws ec2 create-volume --size 10 --region ap-south-1 --availability-zone ap-south-1a --volume-type gp2'; # to create a volume, note the volume ID
          + then you need to create a pod with a volume definition
      + % cp -rf material/kubernetes-course/volumes work/. && cd work
      + % kops create cluster --name=yar-aws.tk --state=s3://<s3bucket> --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=yar-aws.tk; # create cluster
      + % kops update cluster yar-aws.tk --state=s3://<s3bucket> --yes; # launch the cluster
      + % kops get cluster --state=s3://<s3bucket>; # list of clusters on aws
      + % kubectl get node; # wait till nodes are up
      + % aws ec2 create-volume --size 10 --region ap-south-1 --availability-zone ap-south-1a --volume-type gp2; # create volume on aws, gp2 is ssd volume
      + copy volume id and add it in volumeID in volumes/helloworld-with-volume.yml
      + % kubectl create -f volumes/helloworld-with-volume.yml; # deployment created
      + % kubectl get pod; # wait till pod running
      + % kubectl describe pod <podname>; # volume is pointing to aws persistent disk ebs
      + % kubectl exec <podname> -i -t -- bash; # bash inside container
        + p% ls -ahl /myvol/
        + p% echo 'test' > /myvol/myvol.txt
        + p% echo 'test2' > /test.txt
        + p% exit
      + % kubectl get node
      + % kubectl drain <podname> --force; # drain the pod
      + % kubectl get pod; # new pod is being created
      + % kubectl describe pod <podname>; # volume is attached
      + % kubectl exec <podname> -i -t -- bash; # bash inside container
        + p% cat /myvol/myvol.txt; # myvol.txt exists
        + p% cat /test.txt; # file not found
        + p% exit
      + % kubectl delete -f volumes/helloworld-with-volume.yml; # delete deployment
      + % aws ec2 delete-volume --volume-id <volumeid>; # delete volume
      + % kops delete cluster <clustername> --state=s3://<s3bucket>; # info on delete cluster
      + % kops delete cluster <clustername> --state=s3://<s3bucket> --yes; # delete cluster
***** demo: wordpress with volumes
      + volumes
        + kubernetes plugins have the capability to provision storage to you
        + aws plugin can for instance provision storage for you by creating the volumes in aws before attaching them to a node
        + done using StorageClass object, in beta will be stable soon
        + http://kubernetes.io/docs/user-guide/persistent-volumes/ - check documentation
        + you can do auto provisioned volumes via yaml file (e.g wordpress-volumes/storage.yml)
        + next, you can crate a volume claim and specify the size (e.g. wordpress-volumes/pv-claim.yml)
        + and launch pod using volume (e.g. wordpress-db.yml)
      + Important example
        + % cp -rf material/kubernetes-course/wordpress-volumes work/. && cd work
        + % kops create cluster --name=yar-aws.tk --state=s3://<s3bucket> --zones=ap-south-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=yar-aws.tk; # create
        + % kops update cluster yar-aws.tk --state=s3://<s3bucket> --yes; # launch the cluster
        + % aws efs create-file-system --creation-token 1; # token number should be unique; copy FileSystemId, this storage is for web to upload any photos; ebs for database and efs (shared storage) for sharing files; efs is still not available on ap-south-1; try example on EU(Ireland) - eu-west-1(region)/eu-west1a(zone)
        + % aws ec2 describe-instances; # get SubnetId and SecurityGroups.GroupId
        + % aws efs create-mount-target --file-system-id <FileSystemId> --subnet-id <SubnetId> --security-groups <SecurityGroups.GroupId>; # Copy FileSystemId and replace it in wordpress.yml volumes::nfs::server 'ap-south-1a.<FileSystemId>.ap-south-1.amazonaws.com'
        + % kubectl create -f wordpress-volumes/storage.yml; # storageclass created
        + % kubectl create -f wordpress-volumes/pv-claim.yml; # persistentvolumeclaim created
        + % kubectl create -f wordpress-volumes/wordpress-db.yml; # replicationcontroller created
        + % kubectl create -f wordpress-volumes/wordpress-db-service.yml; # service created
        + % kubectl get pvc; # volume is bound; persistent volume claim
        + % kubectl get pod; # wait until pod is running
        + % kubectl describe pod <podname>; # info about pod, check persistent volume claim
        + % kubectl create -f wordpress-volumes/wordpress-web.yml; # deployment created
        + % kubectl create -f wordpress-volumes/wordpress-web-service.yml; # service created
        + create a new record set "wordpress".<dns volume>, alias to elb classic load balancer
        + in browser wordpress.<dns volume> and complete installation; not data is on persistent volume; also has efs to upload media
        + % kubectl get pod; # 2 pods running web requests and 1 pod running db
        + default media doesn't get upload to efs, there is bug. To overcome follow below instructions
        + % kubectl edit deploy/wordpress-deployment
          + under container spec add commands (refer video lecture)
          + ' - command:
                -bash
                - -c
                - chown www-data:www-data /var/www/html/wp-content/uploads && docker-entrypoint.sh apache2-foreground '
        + upload sample image under media, it should work now
        + % kubectl get pod
        + % kubectl delete pod <pod1name>; # pod deleted all pods
        + % kubectl delete pod <pod2name>; # pod deleted all pods
        + % kubectl delete pod <pod3name>; # pod deleted all pods
        + % kubectl get pod; # pod relaunched
        + % kubectl exec <webpod> -i -t -- bash; # ls wp-content/uploads ; # we still have image which is uploaded earlier
        + % kubectl logs <dbpod>; # db started
        + open browser again and check, data should remain
        + when you delete kubectl deployment, services, kops cluster delete efs volume as well
***** pet sets, daemon sets
      + pet sets
        + stateful distributed apps on a kubernetes cluster
        + new feature starting from kubernetes 1.3
        + to be able to run stateful application that need a stable pod hostname (instead of podname-randomstring)
        + your podname will have an index when having multiple instances of a pod (e.g. podname-0 podname-1 and podname-2)
        + a stateful app that requires multiple pods with volumes based on their ordinal number (podname-x) or hostname
        + currently deleting and/or scaling a petset down will not delete the volumes associated with the petset
        + a pet set will allow your stateful app to use DNS to find other peers
          + Cassandra clusters, ElasticSearch clusters, use DNS to find other members of the cluster
          + one running node of your PetSet is called a Pet (e.g. 1 node in cassandra)
          + using petsets you can run for instance 5 cassandra nodes on kubernetes named cassandra-1 until cassandra-5
          + if you don't use PetSets, you would get a dynamic hostname, which you wouldn't be able to use in your configuration files, as the name can always change
          + a pet set will also allow you stateful app to order your startup and teardown of the pets. Instead of randomly terminating one pet (one instance of your app) you'll know which one that will go
          + this is useful if you first need to drain the data from a node before it can be shut down
          + pet sets are still in alpha so will add demo later
***** daemon sets
      + daemon sets
        + daemon sets ensure that every single node in the kubernetes cluster runs the same pod resource, it's useful if you want to ensure that a certain pod is running on every single kubernetes node
        + when a node is added to the cluster, a new pod will be started automatically, when a node is removed the pod will not be rescheduled on another node
        + typical use cases: logging aggregators, monitoring, load balancers/reverse proxies/API gateways, running a daemon that only needs one instance per physical instance
        + take a look at example daemonsets-not-working/daemonset.yml - example is not working/tested (demo not available)
***** demo: resource usage monitoring
      + resource usage monitoring
        + heapster enables container cluster monitoring and performance analysis, it's providing a monitoring platform for kubernetes
        + it's a prerequisite if you want to do pod auto-scaling in kubernetes
        + heapster exports clusters metrics via REST endpoints, you can use different backends with Heapster
        + we will use InfluxDB, but others like Google Cloud Monitoring/Logging and Kafka are also possible
        + visualization (graphs) can be shown using Grafana
        + kubernetes dashboard will also show graphs once monitoring is enabled
        + all these technologies (Heapster, InfluxDB and Grafana) can be started in pods
        + the yaml files can be found on the github repository of heapster
          + https://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb
          + after downloading the repository the whole platform can be deployed using the addon system or by using 'kubectl create -f directory-with-yaml-files/'
      + % git clone https://github.com/kubernetes/heapster.git - heapster repo
      + % ls heapster/deploy/kube-config/influxdb; # kubernetes add on
      + edit 'heapster/deploy/kube-config/influxdb/grafana.yaml' to comment "kubernetes.io/cluster-service: 'true'"; 
      + edit 'heapster/deploy/kube-config/influxdb/heapster.yaml' to comment "kubernetes.io/cluster-service: 'true'"; 
      + edit 'heapster/deploy/kube-config/influxdb/influxdb.yaml' to comment "kubernetes.io/cluster-service: 'true'"; 
      + % minikube start --cpus 1 --memory 1024
      + % kubectl create -f heapster/deploy/kube-config/influxdb/ ; # can do create on directory; deployment and service monitoring-grafana/heapster/monitoring-influxdb created
      + % kubectl create -f deployment/helloworld.yml; # run deployment example
      + % minikube service monitoring-grafana --namespace=kube-system --url
        + open url in browser with login/passwd 'admin'
        + you can cpu/memory and network usage
      + % minikube dashboard --url
        + you can also see cpu/memory usage
***** demo: autoscaling
      + autoscaling
        + horizontal pod autoscaling
        + kubernetes has the possibility to automatically scale pods based on metrics
        + kubernetes can automatically scale a Deployment, Replication Controller or ReplicaSet
        + In kubernetes 1.3 scaling based on CPU usage is possible out of the box; with alpha support, application based metrics are also available (like queries per second or average rquest latency)
        + to enable, the cluster has to be started with the env var ENABLE_CUSTOM_METRICS to true
        + autoscaling will periodically query the utilization for the targeted pods, by default 30sec can be changed with "--horizontal-pod-autoscaler-sync-period" when launching the controller-manager
        + autoscaling will use heapster, the monitoring tool to gather its metrics and make scaling decisions; heapster must be installed and running before autoscaling will work
        + eg:
          + you run a deployment with a pod with a CPU resource request of 200m (200millicpu or 200millicores; 20% of a CPU core of the running node; if the node has 2 cores, it's still 20% of a single core)
          + you introduce auto-scaling at 50% of the CPU usage (which is 100m)
          + horizontal pod autoscaling will increase/decrease pods to maintain a target CPU utilization of 50% (or 100m/10% of a core within this pod)
      + % cp -rf material/kubernetes-course/autoscaling work/. && cd work
      + run minikube and heapster 
      + % kubectl create -f autoscaling/hpa-example.yml; # deployment, service, horizontalpodautoscaler created
      + % kubectl get hpa; # list of hotizontal pod autoscaler
      + % kubectl run -i --tty load-generator --image=busybox /bin/sh
        + p% wget http://hpa-example.default.svc.cluster.local:31001
        + p% cat index.html
        + p% rm index.html
        + p% while true; do wget -q -O- https://hpa-example.default.svc.cluster.local:31001; done; # to run cpu intensive tasks and wait for sometime for autoscale to see
      + % kubectl get pod
      + % kubectl get hpa; # hpa is active
      + % kubectl get pod; # more pods are running due to autoscaling
        + p% ctrl-c after 1 minute
      + % kubectl get hpa; # after sometime hpa autoscale down
      + % kubectl get pod; # less pods are running
**** kubernetes administration
***** the kubernetes master services
***** resource quotas
***** demo: namespace quotas
***** user management
***** demo: adding users
***** networking
***** demo: node maintenance
***** demo: high availability
**** course completion
***** bonus lecture: advanced kubernetes usage course
    
